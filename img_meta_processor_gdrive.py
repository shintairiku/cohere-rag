import os
import io
import json
import traceback
import base64
import hashlib
import gc  # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ç”¨
import signal  # ã‚·ã‚°ãƒŠãƒ«ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ç”¨
import sys
import time
from datetime import datetime

import cohere
import numpy as np
from dotenv import load_dotenv
from google.cloud import storage
from PIL import Image

# Decompression bombå¯¾ç­–: æœ€å¤§ç”»åƒãƒ”ã‚¯ã‚»ãƒ«æ•°ã‚’è¨­å®šï¼ˆç´„500MPï¼‰
Image.MAX_IMAGE_PIXELS = 500_000_000

import google.auth
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseDownload
from drive_scanner import list_files_in_drive_folder # drive_scanner.pyã‚’å†åˆ©ç”¨

load_dotenv()

# --- 1. ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿ã¨æ¤œè¨¼ ---
# ãƒãƒƒãƒãƒ¢ãƒ¼ãƒ‰åˆ¤å®š
BATCH_MODE = os.getenv("BATCH_MODE", "false").lower() == "true"

if BATCH_MODE:
    # ãƒãƒƒãƒãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã€ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆã‚’å–å¾—
    BATCH_TASKS_JSON = os.getenv("BATCH_TASKS", "[]")
    try:
        BATCH_TASKS = json.loads(BATCH_TASKS_JSON)
    except json.JSONDecodeError:
        raise RuntimeError("FATAL: Invalid BATCH_TASKS JSON format")
else:
    # å˜ä¸€ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã€å¾“æ¥é€šã‚Š
    UUID = os.getenv("UUID")
    DRIVE_URL = os.getenv("DRIVE_URL")
    # Cloud Runã‚¸ãƒ§ãƒ–ã§Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‹ã‚‰å—ã‘å–ã£ãŸembed-v4.0ä½¿ç”¨ãƒ•ãƒ©ã‚°
    USE_EMBED_V4 = os.getenv("USE_EMBED_V4", "false").lower() == "true"

GCS_BUCKET_NAME = os.getenv("GCS_BUCKET_NAME")
COHERE_API_KEY = os.getenv("COHERE_API_KEY")
MAX_IMAGE_SIZE_MB = 5  # Cohere APIåˆ¶é™: æœ€å¤§5MB
CHECKPOINT_INTERVAL = 100  # 100ä»¶ã”ã¨ã«é€”ä¸­ä¿å­˜

# ãƒ‡ãƒãƒƒã‚°ç”¨è¨­å®š
DEBUG_MODE = os.getenv("DEBUG_MODE", "false").lower() == "true"
SIMULATE_MEMORY_ERROR_AT = int(os.getenv("SIMULATE_MEMORY_ERROR_AT", "0"))  # æŒ‡å®šã—ãŸç”»åƒç•ªå·ã§ãƒ¡ãƒ¢ãƒªã‚¨ãƒ©ãƒ¼ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
SIMULATE_PROCESSING_ERROR_AT = int(os.getenv("SIMULATE_PROCESSING_ERROR_AT", "0"))  # æŒ‡å®šã—ãŸç”»åƒç•ªå·ã§å‡¦ç†ã‚¨ãƒ©ãƒ¼ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ

if BATCH_MODE:
    if not all([GCS_BUCKET_NAME, COHERE_API_KEY]):
        missing = [
            var for var in ['GCS_BUCKET_NAME', 'COHERE_API_KEY']
            if not os.getenv(var)
        ]
        raise RuntimeError(f"FATAL: Required environment variables are missing: {', '.join(missing)}")
    if not BATCH_TASKS:
        raise RuntimeError("FATAL: No tasks provided in batch mode")
else:
    if not all([GCS_BUCKET_NAME, COHERE_API_KEY, UUID, DRIVE_URL]):
        missing = [
            var for var in ['GCS_BUCKET_NAME', 'COHERE_API_KEY', 'UUID', 'DRIVE_URL']
            if not os.getenv(var)
        ]
        raise RuntimeError(f"FATAL: Required environment variables are missing: {', '.join(missing)}")

# --- 2. ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ– ---
co_client = cohere.Client(COHERE_API_KEY)

# ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã§ã¯ Google Cloud Storage ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–ã—ãªã„
if not DEBUG_MODE:
    storage_client = storage.Client()
else:
    storage_client = None
    print("ğŸ§ª [DEBUG] Skipping Google Cloud Storage client initialization")

MAX_FILE_SIZE_BYTES = MAX_IMAGE_SIZE_MB * 1024 * 1024

def resize_image_if_needed(image_content: bytes, filename: str) -> bytes:
    """
    ç”»åƒã®è§£åƒåº¦ãŒCohere APIåˆ¶é™ã‚’è¶…ãˆã‚‹å ´åˆã€ãƒ”ã‚¯ã‚»ãƒ«æ•°ãƒ™ãƒ¼ã‚¹ã§ãƒªã‚µã‚¤ã‚ºã™ã‚‹ã€‚
    Cohere APIã¯è§£åƒåº¦ãƒ™ãƒ¼ã‚¹ã§åˆ¶é™ã‚’è¡Œã†ãŸã‚ã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã§ã¯ãªããƒ”ã‚¯ã‚»ãƒ«æ•°ã§åˆ¤å®šã€‚
    """
    try:
        # ã¾ãšç”»åƒã¨ã—ã¦èª­ã¿è¾¼ã‚ã‚‹ã‹æ¤œè¨¼
        try:
            img = Image.open(io.BytesIO(image_content))
            # ç”»åƒã‚’èª­ã¿è¾¼ã‚“ã§åŸºæœ¬æƒ…å ±ã‚’ç¢ºèªï¼ˆå®Ÿéš›ã«ãƒ”ã‚¯ã‚»ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€ï¼‰
            img.verify()
            # verifyã¯ç”»åƒã‚’é–‰ã˜ã‚‹ã®ã§ã€å†åº¦é–‹ã
            img = Image.open(io.BytesIO(image_content))
        except Image.DecompressionBombError as e:
            print(f"    âš ï¸  Decompression bomb warning for '{filename}': {e}")
            print(f"       File might be too large or corrupted. Skipping...")
            return None
        except OSError as e:
            print(f"    âš ï¸  Cannot identify image file '{filename}': {e}")
            print(f"       File might not be a valid image or is corrupted. Skipping...")
            return None
        except Exception as e:
            print(f"    âš ï¸  Unexpected error opening image '{filename}': {e}")
            return None
            
        original_width, original_height = img.size
        original_pixels = original_width * original_height
        original_size_mb = len(image_content) / (1024 * 1024)
        
        # æ¥µç«¯ã«å¤§ãã„ç”»åƒã®å ´åˆã¯è­¦å‘Šã‚’å‡ºã—ã¦ã‚¹ã‚­ãƒƒãƒ—
        if original_pixels > 100_000_000:  # 100MPä»¥ä¸Š
            print(f"    âš ï¸  Extremely large image: {original_width}x{original_height} ({original_pixels:,} pixels)")
            print(f"       This image is too large to process safely. Skipping...")
            return None
        
        # Cohere API embed-v4.0ã®è§£åƒåº¦åˆ¶é™: ç´„240ä¸‡ãƒ”ã‚¯ã‚»ãƒ«
        # å®‰å…¨ãƒãƒ¼ã‚¸ãƒ³ã‚’è€ƒæ…®ã—ã¦2.3MP (2,300,000ãƒ”ã‚¯ã‚»ãƒ«) ã‚’ä¸Šé™ã¨ã™ã‚‹
        MAX_PIXELS = 2_300_000
        
        # è§£åƒåº¦ãƒã‚§ãƒƒã‚¯
        if original_pixels <= MAX_PIXELS:
            return image_content
        
        print(f"    ğŸ“ High resolution image detected: {original_width}x{original_height} ({original_pixels:,} pixels > {MAX_PIXELS:,} limit)")
        print(f"       File size: {original_size_mb:.1f}MB")
        
        # RGBAã‚„Pãƒ¢ãƒ¼ãƒ‰ã®ç”»åƒã‚’RGBã«å¤‰æ›
        if img.mode in ('RGBA', 'LA', 'P'):
            background = Image.new('RGB', img.size, (255, 255, 255))
            background.paste(img, mask=img.split()[-1] if 'A' in img.mode else None)
            img = background

        # å¿…è¦ãªã‚¹ã‚±ãƒ¼ãƒ«ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã‚’è¨ˆç®—ï¼ˆãƒ”ã‚¯ã‚»ãƒ«æ•°ãƒ™ãƒ¼ã‚¹ï¼‰
        scale_factor = (MAX_PIXELS / original_pixels) ** 0.5  # é¢ç©æ¯”ã®å¹³æ–¹æ ¹
        
        # æœ€å°ã§ã‚‚0.3å€ã¾ã§ã—ã‹ã‚¹ã‚±ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³ã—ãªã„ï¼ˆå“è³ªä¿æŒã®ãŸã‚ï¼‰
        scale_factor = max(0.3, scale_factor)
        
        new_width = int(original_width * scale_factor)
        new_height = int(original_height * scale_factor)
        new_pixels = new_width * new_height
        
        print(f"    ğŸ”¢ Calculated scale factor: {scale_factor:.3f}")
        print(f"       New resolution: {new_width}x{new_height} ({new_pixels:,} pixels)")
        
        # ãƒªã‚µã‚¤ã‚ºå®Ÿè¡Œ
        resized_img = img.resize((new_width, new_height), Image.Resampling.LANCZOS)
        
        # å“è³ª90ã§ä¿å­˜
        output = io.BytesIO()
        resized_img.save(output, format='JPEG', quality=90, optimize=True)
        resized_data = output.getvalue()
        resized_size_mb = len(resized_data) / (1024 * 1024)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚‚5MBã‚’è¶…ãˆãŸå ´åˆã¯å“è³ªã‚’ä¸‹ã’ã‚‹
        quality = 90
        while len(resized_data) > MAX_FILE_SIZE_BYTES and quality >= 60:
            quality -= 10
            output = io.BytesIO()
            resized_img.save(output, format='JPEG', quality=quality, optimize=True)
            resized_data = output.getvalue()
            resized_size_mb = len(resized_data) / (1024 * 1024)
        
        print(f"    âœ… Successfully resized: {original_size_mb:.1f}MB -> {resized_size_mb:.1f}MB")
        print(f"       Resolution: {original_width}x{original_height} -> {new_width}x{new_height}")
        print(f"       Quality: {quality}")
        
        return resized_data
        
    except Exception as e:
        print(f"    âŒ Resize Error: {e}")
        traceback.print_exc()
        return None

def get_multimodal_embedding(image_bytes: bytes, filename: str, file_index: int = 0, use_embed_v4: bool = False) -> np.ndarray:
    """ç”»åƒãƒ‡ãƒ¼ã‚¿ã¨ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰é‡ã¿ä»˜ã‘ã•ã‚ŒãŸãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆã™ã‚‹"""
    try:
        # ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’æ±ºå®š
        embed_model = "embed-v4.0" if use_embed_v4 else "embed-multilingual-v3.0"
        print(f"    ğŸ”§ Using embedding model: {embed_model}")
        # ãƒ‡ãƒãƒƒã‚°: ãƒ¡ãƒ¢ãƒªã‚¨ãƒ©ãƒ¼ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        if DEBUG_MODE and SIMULATE_MEMORY_ERROR_AT > 0 and file_index == SIMULATE_MEMORY_ERROR_AT:
            print(f"ğŸ§ª [DEBUG] Simulating memory error at file #{file_index}")
            raise MemoryError("Simulated out-of-memory event for debugging")
        
        # ãƒ‡ãƒãƒƒã‚°: å‡¦ç†ã‚¨ãƒ©ãƒ¼ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        if DEBUG_MODE and SIMULATE_PROCESSING_ERROR_AT > 0 and file_index == SIMULATE_PROCESSING_ERROR_AT:
            print(f"ğŸ§ª [DEBUG] Simulating processing error at file #{file_index}")
            raise Exception("Simulated processing error for debugging")
        
        # ãƒ‡ãƒãƒƒã‚°: APIã‚³ã‚¹ãƒˆã‚’å‰Šæ¸›ã™ã‚‹ãŸã‚ã€ãƒ€ãƒŸãƒ¼ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¿”ã™
        if DEBUG_MODE:
            print(f"ğŸ§ª [DEBUG] Returning dummy embedding for '{filename}' (saves API cost)")
            # ãƒ¢ãƒ‡ãƒ«ã«å¿œã˜ãŸæ¬¡å…ƒæ•°ã®ãƒ€ãƒŸãƒ¼ãƒ™ã‚¯ãƒˆãƒ«
            dimensions = 1024 if embed_model == "embed-multilingual-v3.0" else 1024  # embed-v4.0ã‚‚1024æ¬¡å…ƒ
            dummy_vec = np.random.normal(0, 1, dimensions)
            dummy_vec = dummy_vec / np.linalg.norm(dummy_vec)  # æ­£è¦åŒ–
            return dummy_vec
        # 1. ãƒ•ã‚¡ã‚¤ãƒ«åã‚’textã¨ã—ã¦ãƒ™ã‚¯ãƒˆãƒ«åŒ–
        text_response = co_client.embed(
            texts=[filename],
            model=embed_model,
            input_type="search_document"
        )
        text_vec = np.array(text_response.embeddings[0])
        
        # 2. ç”»åƒã‚’imageã¨ã—ã¦ãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼ˆdata URIå½¢å¼ã§é€ä¿¡ï¼‰
        file_extension = filename.lower().split('.')[-1]
        if file_extension in ['jpg', 'jpeg']:
            mime_type = 'jpeg'
        elif file_extension in ['png']:
            mime_type = 'png'
        elif file_extension in ['gif']:
            mime_type = 'gif'
        elif file_extension in ['webp']:
            mime_type = 'webp'
        else:
            mime_type = 'jpeg'
        
        base64_string = base64.b64encode(image_bytes).decode("utf-8")
        data_uri = f"data:image/{mime_type};base64,{base64_string}"
        
        image_response = co_client.embed(
            images=[data_uri],
            model=embed_model,
            input_type="image"
        )
        image_vec = np.array(image_response.embeddings[0])
        
        # 3. ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦wã‚’è¨ˆç®—
        dot_product = np.dot(text_vec, image_vec)
        norm_text = np.linalg.norm(text_vec)
        norm_image = np.linalg.norm(image_vec)
        w = dot_product / (norm_text * norm_image)
        
        # wã‚’0-1ã®ç¯„å›²ã«ã‚¯ãƒªãƒƒãƒ—ï¼ˆè² ã®å€¤ã‚’é¿ã‘ã‚‹ï¼‰
        w = max(0, min(1, w))
        
        # 4. é‡ã¿ä»˜ã‘çµ±åˆãƒ™ã‚¯ãƒˆãƒ«ã‚’è¨ˆç®—
        final_vec = w * text_vec + (1 - w) * image_vec
        # final_vec = image_vec
        
        print(f"    ğŸ“Š Text-Image similarity: {w:.3f} for '{filename}'")
        return final_vec
        
    except Exception as e:
        print(f"    âš ï¸  Warning: Could not generate multimodal embedding for '{filename}'. Skipping. Reason: {e}")
        return None

def load_existing_embeddings(bucket_name: str, uuid: str) -> tuple:
    """æ—¢å­˜ã®embeddingsã¨å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã‚’èª­ã¿è¾¼ã‚€"""
    if DEBUG_MODE:
        print("ğŸ§ª [DEBUG] Skipping existing embeddings check")
        return [], set()
        
    try:
        bucket = storage_client.bucket(bucket_name)
        blob = bucket.blob(f"{uuid}.json")
        
        if blob.exists():
            existing_data = json.loads(blob.download_as_text())
            processed_files = {item['filename'] for item in existing_data}
            print(f"ğŸ“‚ Found existing data with {len(existing_data)} embeddings")
            return existing_data, processed_files
        else:
            print("ğŸ“‚ No existing data found, starting fresh")
            return [], set()
    except Exception as e:
        print(f"âš ï¸  Could not load existing data: {e}")
        return [], set()

def save_checkpoint(bucket_name: str, uuid: str, embeddings: list, is_final: bool = False):
    """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¨ã—ã¦embeddingsã‚’{uuid}.jsonã«ä¿å­˜"""
    if DEBUG_MODE:
        print(f"ğŸ§ª [DEBUG] Skipping save checkpoint ({len(embeddings)} embeddings)")
        return
        
    try:
        from datetime import datetime
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        bucket = storage_client.bucket(bucket_name)
        blob = bucket.blob(f"{uuid}.json")
        blob.upload_from_string(
            json.dumps(embeddings, ensure_ascii=False, indent=2),
            content_type="application/json"
        )
        
        if is_final:
            print(f"âœ… [{current_time}] Final save completed: {len(embeddings)} embeddings saved to gs://{bucket_name}/{uuid}.json")
        else:
            print(f"ğŸ’¾ [{current_time}] Checkpoint saved: {len(embeddings)} embeddings saved to gs://{bucket_name}/{uuid}.json")
            
    except Exception as e:
        print(f"âŒ [{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] Failed to save checkpoint to gs://{bucket_name}/{uuid}.json: {e}")
        traceback.print_exc()

def process_single_uuid(uuid: str, drive_url: str, use_embed_v4: bool = False, all_embeddings: list = None) -> list:
    """å˜ä¸€UUIDã®å‡¦ç†"""
    if all_embeddings is None:
        all_embeddings = []
    
    print(f"ğŸ“‹ Processing UUID: {uuid}")
    print(f"   Drive URL: {drive_url}")
    print(f"   Using Embed Model: {'embed-v4.0' if use_embed_v4 else 'embed-multilingual-v3.0'}")
    print(f"ğŸ” Debug - Looking for file: gs://{GCS_BUCKET_NAME}/{uuid}.json")
    
    try:
        # æ—¢å­˜ã®embeddingsã‚’èª­ã¿è¾¼ã‚€
        existing_embeddings, processed_files = load_existing_embeddings(GCS_BUCKET_NAME, uuid)
        task_embeddings = existing_embeddings.copy()
        
        if DEBUG_MODE:
            # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã§ã¯ãƒ€ãƒŸãƒ¼ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã‚’ä½¿ç”¨
            files_to_process = [
                {'name': f'debug_image_{uuid}_1.jpg', 'id': 'debug_id_1', 'webViewLink': f'https://debug.example.com/{uuid}_1', 'folder_path': '/debug'},
                {'name': f'debug_image_{uuid}_2.png', 'id': 'debug_id_2', 'webViewLink': f'https://debug.example.com/{uuid}_2', 'folder_path': '/debug'}
            ]
            print(f"ğŸ§ª [DEBUG] Using {len(files_to_process)} dummy files for UUID {uuid}")
        else:
            files_to_process = list_files_in_drive_folder(drive_url)
            if not files_to_process:
                print(f"âœ… No processable images found for UUID {uuid}")
                return task_embeddings
        
        # æ—¢ã«å‡¦ç†æ¸ˆã¿ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—
        original_count = len(files_to_process)
        processed_file_keys = {f"{item.get('folder_path', '')}/{item.get('filename', '')}" for item in existing_embeddings}
        
        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¿½åŠ 
        print(f"ğŸ” Debug - Existing embeddings count: {len(existing_embeddings)}")
        print(f"ğŸ” Debug - Processed file keys: {processed_file_keys}")
        print(f"ğŸ” Debug - Files to process (before filter): {[f['name'] for f in files_to_process[:5]]}...")  # æœ€åˆã®5ä»¶ã®ã¿è¡¨ç¤º
        
        files_to_process = [f for f in files_to_process if f"{f.get('folder_path', '')}/{f['name']}" not in processed_file_keys]
        skipped_count = original_count - len(files_to_process)
        
        if not files_to_process:
            print(f"âœ… All {skipped_count} images already processed for UUID {uuid}")
            return task_embeddings

        print(f"Found {len(files_to_process)} new images to process for UUID {uuid} (skipping {skipped_count} already processed)")
        
        if not DEBUG_MODE:
            print("Initializing Google Drive service...")
            drive_creds, _ = google.auth.default(scopes=['https://www.googleapis.com/auth/drive.readonly'])
            drive_service = build('drive', 'v3', credentials=drive_creds)
        else:
            drive_service = None
            print("ğŸ§ª [DEBUG] Skipping Google Drive service initialization")
        
        # å‡¦ç†é–‹å§‹æ™‚åˆ»ã‚’è¨˜éŒ²
        start_time = datetime.now()
        
        for i, file_info in enumerate(files_to_process, 1):
            print(f"    ({i}/{len(files_to_process)}) Processing: {file_info['name'][:50]}...")
            
            try:
                if DEBUG_MODE:
                    # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã§ã¯PILã§ãƒ€ãƒŸãƒ¼ç”»åƒã‚’ç”Ÿæˆ
                    print("      ğŸ§ª [DEBUG] Using dummy image data (skipping actual download)")
                    dummy_img = Image.new('RGB', (100, 100), color='red')
                    output = io.BytesIO()
                    dummy_img.save(output, format='JPEG')
                    image_content = output.getvalue()
                else:
                    # 1. Download image from Google Drive
                    request = drive_service.files().get_media(fileId=file_info['id'])
                    fh = io.BytesIO()
                    downloader = MediaIoBaseDownload(fh, request)
                    done = False
                    while not done:
                        _, done = downloader.next_chunk()
                    image_content = fh.getvalue()
                
                # 2. Resize if necessary
                resized_content = resize_image_if_needed(image_content, file_info['name'])
                if resized_content is None:
                    print(f"      â­ï¸  Skipping due to resize failure")
                    continue

                # 3. Get multimodal embedding
                embedding = get_multimodal_embedding(resized_content, file_info['name'], i, use_embed_v4)
                if embedding is not None:
                    result_data = {
                        "filename": file_info['name'],
                        "filepath": file_info['webViewLink'],
                        "folder_path": file_info['folder_path'],
                        "embedding": embedding.tolist()
                    }
                    task_embeddings.append(result_data)
                    
                    # 100ä»¶ã”ã¨ã«é€”ä¸­ä¿å­˜ã‚’å®Ÿè¡Œ
                    if i % CHECKPOINT_INTERVAL == 0:
                        print(f"ğŸ“Œ Checkpoint reached: processed {i}/{len(files_to_process)} files")
                        save_checkpoint(GCS_BUCKET_NAME, uuid, task_embeddings, is_final=False)
                        print(f"ğŸ’¾ Checkpoint saved: {len(task_embeddings)} embeddings")
                    
                    # APIåˆ¶é™å¯¾ç­–ï¼šç”»åƒå‡¦ç†ã®é–“éš”ã‚’ç©ºã‘ã‚‹ï¼ˆç¾åœ¨ã¯ç„¡åŠ¹åŒ–ï¼‰
                    # if not DEBUG_MODE and i < len(files_to_process):
                    #     print(f"      â±ï¸  Waiting 15 seconds before next API call...")
                    #     time.sleep(15)  # 15ç§’å¾…æ©Ÿï¼ˆ5å›/åˆ†åˆ¶é™å¯¾ç­–ï¼‰

            except Exception as e:
                print(f"      âŒ Error processing {file_info['name']}: {e}")
                # å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚¨ãƒ©ãƒ¼ã¯ç¶™ç¶š
                continue
        
        # ã‚¿ã‚¹ã‚¯å®Œäº†å¾Œã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
        if task_embeddings != existing_embeddings:
            elapsed_total = (datetime.now() - start_time).total_seconds()
            print(f"   â±ï¸  Processing time for UUID {uuid}: {elapsed_total:.1f} seconds")
            save_checkpoint(GCS_BUCKET_NAME, uuid, task_embeddings, is_final=True)
            print(f"   âœ… Saved {len(task_embeddings)} embeddings for UUID {uuid}")
        
        return task_embeddings
        
    except Exception as e:
        print(f"   âŒ Error processing UUID {uuid}: {e}")
        traceback.print_exc()
        # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚ä¿å­˜ã‚’è©¦ã¿ã‚‹
        if task_embeddings:
            try:
                save_checkpoint(GCS_BUCKET_NAME, uuid, task_embeddings, is_final=False)
                print(f"   ğŸ’¾ Emergency save for UUID {uuid}: {len(task_embeddings)} embeddings")
            except Exception as save_error:
                print(f"   âŒ Emergency save failed for UUID {uuid}: {save_error}")
        raise e


def main():
    """Cloud Runã‚¸ãƒ§ãƒ–ã¨ã—ã¦å®Ÿè¡Œã•ã‚Œã‚‹ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    
    # ç’°å¢ƒå¤‰æ•°ã‚’è¡¨ç¤º
    print("ğŸ”§ Environment Variables:")
    env_vars = [
        "GCS_BUCKET_NAME", "COHERE_API_KEY", "UUID", "DRIVE_URL", 
        "USE_EMBED_V4", "BATCH_MODE", "BATCH_TASKS", "DEBUG_MODE"
    ]
    for var in env_vars:
        value = os.getenv(var, "NOT_SET")
        if var == "COHERE_API_KEY" and value != "NOT_SET":
            # APIã‚­ãƒ¼ã¯æœ€åˆã®10æ–‡å­—ã®ã¿è¡¨ç¤º
            value = f"{value[:10]}..." if len(value) > 10 else value
        elif var == "BATCH_TASKS" and value != "NOT_SET":
            # BATCH_TASKSã¯é•·ã„ã®ã§é•·ã•ã®ã¿è¡¨ç¤º
            value = f"[{len(value)} characters]" if value else "EMPTY"
        print(f"  {var}: {value}")
    print()
    
    if BATCH_MODE:
        print("===================================================")
        print(f"  Starting BATCH Vectorization Job")
        print(f"  Number of tasks: {len(BATCH_TASKS)}")
        print(f"  Checkpoint Mode: Every {CHECKPOINT_INTERVAL} files + error handling")
        print("===================================================")
        
        total_processed = 0
        total_errors = 0
        
        for i, task in enumerate(BATCH_TASKS, 1):
            uuid = task.get('uuid')
            drive_url = task.get('drive_url')
            company_name = task.get('company_name', '')
            use_embed_v4 = task.get('use_embed_v4', False)
            
            print(f"\nğŸ“‹ Task {i}/{len(BATCH_TASKS)}: {company_name} (UUID: {uuid})")
            
            try:
                process_single_uuid(uuid, drive_url, use_embed_v4)
                total_processed += 1
                print(f"âœ… Task {i} completed successfully")
                
                # ã‚¿ã‚¹ã‚¯é–“ã®å¾…æ©Ÿï¼ˆAPIåˆ¶é™å¯¾ç­–ï¼‰ï¼ˆç¾åœ¨ã¯ç„¡åŠ¹åŒ–ï¼‰
                # if i < len(BATCH_TASKS):
                #     print(f"â±ï¸  Waiting 30 seconds before next task...")
                #     time.sleep(30)  # ã‚¿ã‚¹ã‚¯é–“ã¯30ç§’å¾…æ©Ÿ
                    
            except Exception as e:
                print(f"âŒ Task {i} failed: {e}")
                total_errors += 1
                # ã‚¿ã‚¹ã‚¯ãŒå¤±æ•—ã—ã¦ã‚‚æ¬¡ã®ã‚¿ã‚¹ã‚¯ã‚’ç¶™ç¶š
                continue
        
        print(f"\nğŸ‰ Batch job completed: {total_processed} successful, {total_errors} failed")
    else:
        # å˜ä¸€ãƒ¢ãƒ¼ãƒ‰ï¼ˆå¾“æ¥é€šã‚Šï¼‰
        print("===================================================")
        print(f"  Starting SINGLE Vectorization Job")
        print(f"  UUID: {UUID}")
        print(f"  Drive URL: {DRIVE_URL}")
        print(f"  Use Embed V4: {USE_EMBED_V4}")
        print(f"  Checkpoint Mode: Every {CHECKPOINT_INTERVAL} files + error handling")
        print("===================================================")
        
        all_embeddings = []  # ã‚°ãƒ­ãƒ¼ãƒãƒ«ã«å‚ç…§ã§ãã‚‹ã‚ˆã†ã«æœ€åˆã«åˆæœŸåŒ–
        
        # ã‚·ã‚°ãƒŠãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®è¨­å®š
        def signal_handler(signum, frame):
            """ã‚·ã‚°ãƒŠãƒ«å—ä¿¡æ™‚ã®å‡¦ç†"""
            print(f"\nâš ï¸  Signal {signum} received. Attempting to save current progress...")
            if all_embeddings:
                try:
                    save_checkpoint(GCS_BUCKET_NAME, UUID, all_embeddings, is_final=False)
                    print(f"âœ… Emergency save successful: {len(all_embeddings)} embeddings saved")
                except Exception as e:
                    print(f"âŒ Emergency save failed: {e}")
            sys.exit(1)
        
        # SIGTERMï¼ˆCloud Runã‹ã‚‰ã®çµ‚äº†ã‚·ã‚°ãƒŠãƒ«ï¼‰ã¨SIGINTï¼ˆCtrl+Cï¼‰ã‚’æ•æ‰
        signal.signal(signal.SIGTERM, signal_handler)
        signal.signal(signal.SIGINT, signal_handler)
        
        print("===================================================")
        print(f"  Starting Vectorization Job for UUID: {UUID}")
        print(f"  Target Drive URL: {DRIVE_URL}")
        print(f"  Using Embed Model: {'embed-v4.0' if USE_EMBED_V4 else 'embed-multilingual-v3.0'}")
        print(f"  Checkpoint Mode: Save on error only")
        print("===================================================")
        
        all_embeddings = process_single_uuid(UUID, DRIVE_URL, USE_EMBED_V4)
        print("ğŸ‰ Single job finished successfully.")

if __name__ == "__main__":
    main()

