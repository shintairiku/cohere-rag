import os
import io
import json
import traceback
import signal
import sys
from datetime import datetime

import numpy as np
from dotenv import load_dotenv
from google.cloud import storage
from PIL import Image as PILImage

from embedding_providers import get_embedding_provider

# Decompression bombÂØæÁ≠ñ: ÊúÄÂ§ßÁîªÂÉè„Éî„ÇØ„Çª„É´Êï∞„ÇíË®≠ÂÆöÔºàÁ¥Ñ500MPÔºâ
PILImage.MAX_IMAGE_PIXELS = 500_000_000

import google.auth
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseDownload
from drive_scanner import list_files_in_drive_folder

load_dotenv()

# --- 1. Áí∞Â¢ÉÂ§âÊï∞„ÅÆË™≠„ÅøËæº„Åø„Å®Ê§úË®º ---
BATCH_MODE = os.getenv("BATCH_MODE", "false").lower() == "true"

if BATCH_MODE:
    BATCH_TASKS_JSON = os.getenv("BATCH_TASKS", "[]")
    try:
        BATCH_TASKS = json.loads(BATCH_TASKS_JSON)
    except json.JSONDecodeError:
        raise RuntimeError("FATAL: Invalid BATCH_TASKS JSON format")
else:
    UUID = os.getenv("UUID")
    DRIVE_URL = os.getenv("DRIVE_URL")
    USE_EMBED_V4 = os.getenv("USE_EMBED_V4", "false").lower() == "true"

GCS_BUCKET_NAME = os.getenv("GCS_BUCKET_NAME")
COHERE_API_KEY = os.getenv("COHERE_API_KEY")
GCP_PROJECT_ID = os.getenv("GCP_PROJECT_ID")
GCP_REGION = os.getenv("GCP_REGION", "asia-northeast1")
VERTEX_MULTIMODAL_MODEL = os.getenv("VERTEX_MULTIMODAL_MODEL", "multimodalembedding@001")
EMBEDDING_PROVIDER = os.getenv("EMBEDDING_PROVIDER", "vertex_ai").lower()
MAX_IMAGE_SIZE_MB = 5
CHECKPOINT_INTERVAL = 100

# „Éá„Éê„ÉÉ„Ç∞Áî®Ë®≠ÂÆö
DEBUG_MODE = os.getenv("DEBUG_MODE", "false").lower() == "true"
SIMULATE_MEMORY_ERROR_AT = int(os.getenv("SIMULATE_MEMORY_ERROR_AT", "0"))
SIMULATE_PROCESSING_ERROR_AT = int(os.getenv("SIMULATE_PROCESSING_ERROR_AT", "0"))

if BATCH_MODE:
    required_vars = ['GCS_BUCKET_NAME', 'GCP_PROJECT_ID']
    missing = [var for var in required_vars if not os.getenv(var)]
    if missing:
        raise RuntimeError(f"FATAL: Required environment variables are missing: {', '.join(missing)}")
    if not BATCH_TASKS:
        raise RuntimeError("FATAL: No tasks provided in batch mode")
else:
    required_vars = ['GCS_BUCKET_NAME', 'GCP_PROJECT_ID', 'UUID', 'DRIVE_URL']
    missing = [var for var in required_vars if not os.getenv(var)]
    if missing:
        raise RuntimeError(f"FATAL: Required environment variables are missing: {', '.join(missing)}")

if EMBEDDING_PROVIDER == "cohere" and not COHERE_API_KEY:
    raise RuntimeError("FATAL: COHERE_API_KEY must be set when EMBEDDING_PROVIDER=cohere")

if not DEBUG_MODE:
    storage_client = storage.Client()
else:
    storage_client = None
    print("üß™ [DEBUG] Skipping Google Cloud Storage client initialization")

MAX_FILE_SIZE_BYTES = MAX_IMAGE_SIZE_MB * 1024 * 1024

def resize_image_if_needed(image_content: bytes, filename: str) -> bytes:
    """
    ÁîªÂÉè„ÅÆËß£ÂÉèÂ∫¶„ÅåÂüã„ÇÅËæº„ÅøAPI„ÅÆÂà∂Èôê„ÇíË∂Ö„Åà„ÇãÂ†¥Âêà„ÄÅ„Éî„ÇØ„Çª„É´Êï∞„Éô„Éº„Çπ„Åß„É™„Çµ„Ç§„Ç∫„Åô„Çã„ÄÇ
    """
    try:
        try:
            img = PILImage.open(io.BytesIO(image_content))
            img.verify()
            img = PILImage.open(io.BytesIO(image_content))
        except PILImage.DecompressionBombError as e:
            print(f"    ‚ö†Ô∏è  Decompression bomb warning for '{filename}': {e}")
            print(f"       File might be too large or corrupted. Skipping...")
            return None
        except OSError as e:
            print(f"    ‚ö†Ô∏è  Cannot identify image file '{filename}': {e}")
            print(f"       File might not be a valid image or is corrupted. Skipping...")
            return None
        except Exception as e:
            print(f"    ‚ö†Ô∏è  Unexpected error opening image '{filename}': {e}")
            return None
            
        original_width, original_height = img.size
        original_pixels = original_width * original_height
        original_size_mb = len(image_content) / (1024 * 1024)
        
        if original_pixels > 100_000_000:
            print(f"    ‚ö†Ô∏è  Extremely large image: {original_width}x{original_height} ({original_pixels:,} pixels)")
            print(f"       This image is too large to process safely. Skipping...")
            return None
        
        MAX_PIXELS = 2_300_000
        
        if original_pixels <= MAX_PIXELS:
            return image_content
        
        print(f"    üìè High resolution image detected: {original_width}x{original_height} ({original_pixels:,} pixels > {MAX_PIXELS:,} limit)")
        print(f"       File size: {original_size_mb:.1f}MB")
        
        if img.mode in ('RGBA', 'LA', 'P'):
            background = PILImage.new('RGB', img.size, (255, 255, 255))
            background.paste(img, mask=img.split()[-1] if 'A' in img.mode else None)
            img = background

        scale_factor = (MAX_PIXELS / original_pixels) ** 0.5
        scale_factor = max(0.3, scale_factor)
        
        new_width = int(original_width * scale_factor)
        new_height = int(original_height * scale_factor)
        new_pixels = new_width * new_height
        
        print(f"    üî¢ Calculated scale factor: {scale_factor:.3f}")
        print(f"       New resolution: {new_width}x{new_height} ({new_pixels:,} pixels)")
        
        resized_img = img.resize((new_width, new_height), PILImage.Resampling.LANCZOS)
        
        output = io.BytesIO()
        resized_img.save(output, format='JPEG', quality=90, optimize=True)
        resized_data = output.getvalue()
        resized_size_mb = len(resized_data) / (1024 * 1024)
        
        quality = 90
        while len(resized_data) > MAX_FILE_SIZE_BYTES and quality >= 60:
            quality -= 10
            output = io.BytesIO()
            resized_img.save(output, format='JPEG', quality=quality, optimize=True)
            resized_data = output.getvalue()
            resized_size_mb = len(resized_data) / (1024 * 1024)
        
        print(f"    ‚úÖ Successfully resized: {original_size_mb:.1f}MB -> {resized_size_mb:.1f}MB")
        print(f"       Resolution: {original_width}x{original_height} -> {new_width}x{new_height}")
        print(f"       Quality: {quality}")
        
        return resized_data
        
    except Exception as e:
        print(f"    ‚ùå Resize Error: {e}")
        traceback.print_exc()
        return None

def get_multimodal_embedding(image_bytes: bytes, filename: str, file_index: int = 0, use_embed_v4: bool = False) -> np.ndarray:
    """ÁîªÂÉè„Éá„Éº„Çø„Å®„Éï„Ç°„Ç§„É´Âêç„Åã„ÇâÈáç„Åø‰ªò„Åë„Åï„Çå„Åü„Éô„ÇØ„Éà„É´„ÇíÁîüÊàê„Åô„Çã"""
    try:
        if DEBUG_MODE and SIMULATE_MEMORY_ERROR_AT > 0 and file_index == SIMULATE_MEMORY_ERROR_AT:
            print(f"üß™ [DEBUG] Simulating memory error at file #{file_index}")
            raise MemoryError("Simulated out-of-memory event for debugging")
        
        if DEBUG_MODE and SIMULATE_PROCESSING_ERROR_AT > 0 and file_index == SIMULATE_PROCESSING_ERROR_AT:
            print(f"üß™ [DEBUG] Simulating processing error at file #{file_index}")
            raise Exception("Simulated processing error for debugging")
        
        provider = get_embedding_provider()
        embedding = provider.embed_multimodal(
            text=filename,
            image_bytes=image_bytes,
            use_embed_v4=use_embed_v4,
        )
        return embedding
    
    except Exception as e:
        print(f"    ‚ö†Ô∏è  Warning: Could not generate multimodal embedding for '{filename}'. Skipping. Reason: {e}")
        traceback.print_exc()
        return None

def load_existing_embeddings(bucket_name: str, uuid: str) -> tuple:
    """Êó¢Â≠ò„ÅÆembeddings„Å®Âá¶ÁêÜÊ∏à„Åø„Éï„Ç°„Ç§„É´„É™„Çπ„Éà„ÇíË™≠„ÅøËæº„ÇÄ"""
    if DEBUG_MODE:
        print("üß™ [DEBUG] Skipping existing embeddings check")
        return [], set()
        
    try:
        bucket = storage_client.bucket(bucket_name)
        blob = bucket.blob(f"{uuid}.json")
        
        if blob.exists():
            existing_data = json.loads(blob.download_as_text())
            processed_files = {item['filename'] for item in existing_data}
            print(f"üìÇ Found existing data with {len(existing_data)} embeddings")
            return existing_data, processed_files
        else:
            print("üìÇ No existing data found, starting fresh")
            return [], set()
    except Exception as e:
        print(f"‚ö†Ô∏è  Could not load existing data: {e}")
        return [], set()

def save_checkpoint(bucket_name: str, uuid: str, embeddings: list, is_final: bool = False):
    """„ÉÅ„Çß„ÉÉ„ÇØ„Éù„Ç§„É≥„Éà„Å®„Åó„Å¶embeddings„Çí{uuid}.json„Å´‰øùÂ≠ò"""
    if DEBUG_MODE:
        print(f"üß™ [DEBUG] Skipping save checkpoint ({len(embeddings)} embeddings)")
        return
        
    try:
        from datetime import datetime
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        bucket = storage_client.bucket(bucket_name)
        blob = bucket.blob(f"{uuid}.json")
        blob.upload_from_string(
            json.dumps(embeddings, ensure_ascii=False, indent=2),
            content_type="application/json"
        )
        
        if is_final:
            print(f"‚úÖ [{current_time}] Final save completed: {len(embeddings)} embeddings saved to gs://{bucket_name}/{uuid}.json")
        else:
            print(f"üíæ [{current_time}] Checkpoint saved: {len(embeddings)} embeddings saved to gs://{bucket_name}/{uuid}.json")
            
    except Exception as e:
        print(f"‚ùå [{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] Failed to save checkpoint to gs://{bucket_name}/{uuid}.json: {e}")
        traceback.print_exc()

def calculate_diff(drive_files: list, existing_embeddings: list) -> tuple:
    """
    Google Drive„Å®„Éô„ÇØ„Éà„É´„Éï„Ç°„Ç§„É´„ÅÆDrive„Å®„Éô„ÇØ„Éà„É´„Éï„Ç°„Ç§„É´„ÅÆÂ∑ÆÂàÜ„ÇíË®àÁÆó„Åô„Çã
    
    Args:
        drive_files: Google Drive„Åã„ÇâÂèñÂæó„Åó„Åü„Éï„Ç°„Ç§„É´„É™„Çπ„Éà
        existing_embeddings: Êó¢Â≠ò„ÅÆ„Éô„ÇØ„Éà„É´„Éá„Éº„Çø
        
    Returns:
        (files_to_add, files_to_delete): ËøΩÂä†ÂØæË±°„Å®ÂâäÈô§ÂØæË±°„ÅÆ„Çø„Éó„É´
    """
    # Google Drive„ÅÆÁèæÂú®„ÅÆ„Éï„Ç°„Ç§„É´„Çª„ÉÉ„ÉàÔºà„Éï„É´„Éë„Çπ„ÅßÁÆ°ÁêÜÔºâ
    drive_file_keys = {f"{f.get('folder_path', '')}/{f['name']}" for f in drive_files}
    
    # „Éô„ÇØ„Éà„É´„Éï„Ç°„Ç§„É´„ÅÆÊó¢Â≠ò„Éï„Ç°„Ç§„É´„Çª„ÉÉ„ÉàÔºà„Éï„É´„Éë„Çπ„ÅßÁÆ°ÁêÜÔºâ
    vector_file_keys = {f"{item.get('folder_path', '')}/{item.get('filename', '')}" for item in existing_embeddings}
    
    # ËøΩÂä†ÂØæË±°: Drive„Å´„ÅÇ„Çã„Åå„Éô„ÇØ„Éà„É´„Å´„Å™„ÅÑ
    keys_to_add = drive_file_keys - vector_file_keys
    
    # ÂâäÈô§ÂØæË±°: „Éô„ÇØ„Éà„É´„Å´„ÅÇ„Çã„ÅåDrive„Å´„Å™„ÅÑ
    keys_to_delete = vector_file_keys - drive_file_keys
    
    # ËøΩÂä†ÂØæË±°„ÅÆ„Éï„Ç°„Ç§„É´ÊÉÖÂ†±„ÇíÊäΩÂá∫
    files_to_add = [f for f in drive_files if f"{f.get('folder_path', '')}/{f['name']}" in keys_to_add]
    
    print(f"\nüìä Diff Analysis Results:")
    print(f"   Drive files: {len(drive_file_keys)}")
    print(f"   Vector files: {len(vector_file_keys)}")
    print(f"   Files to ADD: {len(files_to_add)}")
    print(f"   Files to DELETE: {len(keys_to_delete)}")
    
    if keys_to_delete:
        print(f"\nüóëÔ∏è  Files to be deleted from vector storage:")
        for key in list(keys_to_delete)[:10]:  # ÊúÄÂàù„ÅÆ10‰ª∂„ÅÆ„ÅøË°®Á§∫
            print(f"     - {key}")
        if len(keys_to_delete) > 10:
            print(f"     ... and {len(keys_to_delete) - 10} more")
    
    return files_to_add, keys_to_delete

def remove_deleted_files(existing_embeddings: list, keys_to_delete: set) -> list:
    """
    „Éô„ÇØ„Éà„É´„Éï„Ç°„Ç§„É´„Åã„ÇâÂâäÈô§ÂØæË±°„ÅÆ„Éï„Ç°„Ç§„É´„ÇíÈô§Â§ñ„Åô„Çã
    
    Args:
        existing_embeddings: Êó¢Â≠ò„ÅÆ„Éô„ÇØ„Éà„É´„Éá„Éº„Çø
        keys_to_delete: ÂâäÈô§ÂØæË±°„ÅÆ„Éï„Ç°„Ç§„É´„Ç≠„Éº„Çª„ÉÉ„Éà
        
    Returns:
        ÂâäÈô§Âá¶ÁêÜÂæå„ÅÆ„Éô„ÇØ„Éà„É´„Éá„Éº„Çø
    """
    if not keys_to_delete:
        return existing_embeddings
    
    original_count = len(existing_embeddings)
    
    # ÂâäÈô§ÂØæË±°‰ª•Â§ñ„ÇíÊÆã„Åô
    filtered_embeddings = [
        item for item in existing_embeddings
        if f"{item.get('folder_path', '')}/{item.get('filename', '')}" not in keys_to_delete
    ]
    
    deleted_count = original_count - len(filtered_embeddings)
    
    print(f"\nüóëÔ∏è  Deletion completed:")
    print(f"   Original: {original_count} embeddings")
    print(f"   Deleted: {deleted_count} embeddings")
    print(f"   Remaining: {len(filtered_embeddings)} embeddings")
    
    return filtered_embeddings

def process_single_uuid(uuid: str, drive_url: str, use_embed_v4: bool = False, all_embeddings: list = None) -> list:
    """Âçò‰∏ÄUUID„ÅÆÂá¶ÁêÜÔºàÂ∑ÆÂàÜÊ§úÂá∫„ÉªÂâäÈô§Ê©üËÉΩ‰ªò„ÅçÔºâ"""
    if all_embeddings is None:
        all_embeddings = []
    
    print(f"üìã Processing UUID: {uuid}")
    print(f"   Drive URL: {drive_url}")
    print(f"   Using Embed Model: {'embed-v4.0' if use_embed_v4 else 'embed-multilingual-v3.0'}")
    print(f"üîç Debug - Looking for file: gs://{GCS_BUCKET_NAME}/{uuid}.json")
    
    try:
        # Êó¢Â≠ò„ÅÆembeddings„ÇíË™≠„ÅøËæº„ÇÄ
        existing_embeddings, _ = load_existing_embeddings(GCS_BUCKET_NAME, uuid)
        
        if DEBUG_MODE:
            # „Éá„Éê„ÉÉ„Ç∞„É¢„Éº„Éâ„Åß„ÅØ„ÉÄ„Éü„Éº„ÅÆ„Éï„Ç°„Ç§„É´„É™„Çπ„Éà„Çí‰ΩøÁî®
            drive_files = [
                {'name': f'debug_image_{uuid}_1.jpg', 'id': 'debug_id_1', 'webViewLink': f'https://debug.example.com/{uuid}_1', 'folder_path': '/debug'},
                {'name': f'debug_image_{uuid}_2.png', 'id': 'debug_id_2', 'webViewLink': f'https://debug.example.com/{uuid}_2', 'folder_path': '/debug'}
            ]
            print(f"üß™ [DEBUG] Using {len(drive_files)} dummy files for UUID {uuid}")
        else:
            # Google Drive„Åã„ÇâÁèæÂú®„ÅÆ„Éï„Ç°„Ç§„É´„É™„Çπ„Éà„ÇíÂèñÂæó
            drive_files = list_files_in_drive_folder(drive_url)
            if not drive_files:
                print(f"‚ö†Ô∏è  No files found in Google Drive for UUID {uuid}")
                # Drive„Å´„Éï„Ç°„Ç§„É´„Åå„Å™„ÅÑÂ†¥Âêà„ÄÅÊó¢Â≠ò„ÅÆ„Éô„ÇØ„Éà„É´„Éï„Ç°„Ç§„É´„ÇíÂÖ®ÂâäÈô§
                if existing_embeddings:
                    print(f"üóëÔ∏è  Removing all {len(existing_embeddings)} embeddings (Drive is empty)")
                    save_checkpoint(GCS_BUCKET_NAME, uuid, [], is_final=True)
                return []
        
        # Â∑ÆÂàÜ„ÇíË®àÁÆó
        files_to_add, keys_to_delete = calculate_diff(drive_files, existing_embeddings)
        
        # ÂâäÈô§Âá¶ÁêÜ„ÇíÂÆüË°å
        task_embeddings = remove_deleted_files(existing_embeddings, keys_to_delete)
        
        # ÂâäÈô§„ÅåÁô∫Áîü„Åó„ÅüÂ†¥Âêà„ÅØÂç≥Â∫ß„Å´‰øùÂ≠ò
        if keys_to_delete:
            save_checkpoint(GCS_BUCKET_NAME, uuid, task_embeddings, is_final=False)
            print(f"üíæ Saved after deletion: {len(task_embeddings)} embeddings")
        
        # ËøΩÂä†ÂØæË±°„Åå„Å™„ÅÑÂ†¥Âêà„ÅØÁµÇ‰∫Ü
        if not files_to_add:
            print(f"‚úÖ No new files to process for UUID {uuid}")
            if keys_to_delete:
                # ÂâäÈô§„ÅÆ„ÅøÁô∫Áîü„Åó„ÅüÂ†¥Âêà„ÅØÊúÄÁµÇ‰øùÂ≠ò
                save_checkpoint(GCS_BUCKET_NAME, uuid, task_embeddings, is_final=True)
            return task_embeddings
        
        print(f"\nüìù Processing {len(files_to_add)} new files...")
        
        if not DEBUG_MODE:
            print("Initializing Google Drive service...")
            drive_creds, _ = google.auth.default(scopes=['https://www.googleapis.com/auth/drive.readonly'])
            drive_service = build('drive', 'v3', credentials=drive_creds)
        else:
            drive_service = None
            print("üß™ [DEBUG] Skipping Google Drive service initialization")
        
        start_time = datetime.now()
        
        for i, file_info in enumerate(files_to_add, 1):
            print(f"    ({i}/{len(files_to_add)}) Processing: {file_info['name'][:50]}...")
            
            try:
                if DEBUG_MODE:
                    print("      üß™ [DEBUG] Using dummy image data (skipping actual download)")
                    dummy_img = PILImage.new('RGB', (100, 100), color='red')
                    output = io.BytesIO()
                    dummy_img.save(output, format='JPEG')
                    image_content = output.getvalue()
                else:
                    request = drive_service.files().get_media(fileId=file_info['id'])
                    fh = io.BytesIO()
                    downloader = MediaIoBaseDownload(fh, request)
                    done = False
                    while not done:
                        _, done = downloader.next_chunk()
                    image_content = fh.getvalue()
                
                resized_content = resize_image_if_needed(image_content, file_info['name'])
                if resized_content is None:
                    print(f"      ‚≠ïÔ∏è  Skipping due to resize failure")
                    continue

                embedding = get_multimodal_embedding(resized_content, file_info['name'], i, use_embed_v4)
                if embedding is not None:
                    result_data = {
                        "filename": file_info['name'],
                        "filepath": file_info['webViewLink'],
                        "folder_path": file_info['folder_path'],
                        "embedding": embedding.tolist()
                    }
                    task_embeddings.append(result_data)
                    
                    if i % CHECKPOINT_INTERVAL == 0:
                        print(f"üìå Checkpoint reached: processed {i}/{len(files_to_add)} files")
                        save_checkpoint(GCS_BUCKET_NAME, uuid, task_embeddings, is_final=False)
                        print(f"üíæ Checkpoint saved: {len(task_embeddings)} embeddings")

            except Exception as e:
                print(f"      ‚ùå Error processing {file_info['name']}: {e}")
                continue
        
        # „Çø„Çπ„ÇØÂÆå‰∫ÜÂæå„Å´„Éï„Ç°„Ç§„É´„Çí‰øùÂ≠ò
        if task_embeddings != existing_embeddings or keys_to_delete:
            elapsed_total = (datetime.now() - start_time).total_seconds()
            print(f"   ‚è±Ô∏è  Processing time for UUID {uuid}: {elapsed_total:.1f} seconds")
            save_checkpoint(GCS_BUCKET_NAME, uuid, task_embeddings, is_final=True)
            print(f"   ‚úÖ Saved {len(task_embeddings)} embeddings for UUID {uuid}")
            print(f"   üìä Changes: +{len(files_to_add)} files, -{len(keys_to_delete)} files")
        
        return task_embeddings
        
    except Exception as e:
        print(f"   ‚ùå Error processing UUID {uuid}: {e}")
        traceback.print_exc()
        if task_embeddings:
            try:
                save_checkpoint(GCS_BUCKET_NAME, uuid, task_embeddings, is_final=False)
                print(f"   üíæ Emergency save for UUID {uuid}: {len(task_embeddings)} embeddings")
            except Exception as save_error:
                print(f"   ‚ùå Emergency save failed for UUID {uuid}: {save_error}")
        raise e


def main():
    """Cloud Run„Ç∏„Éß„Éñ„Å®„Åó„Å¶ÂÆüË°å„Åï„Çå„Çã„É°„Ç§„É≥Èñ¢Êï∞"""
    
    print("üîß Environment Variables:")
    env_vars = [
        "GCS_BUCKET_NAME", "GCP_PROJECT_ID", "GCP_REGION", "VERTEX_MULTIMODAL_MODEL",
        "EMBEDDING_PROVIDER", "COHERE_API_KEY",
        "UUID", "DRIVE_URL", "USE_EMBED_V4", "BATCH_MODE", "BATCH_TASKS", "DEBUG_MODE"
    ]
    for var in env_vars:
        value = os.getenv(var, "NOT_SET")
        if var == "COHERE_API_KEY" and value != "NOT_SET":
            value = f"{value[:10]}..." if len(value) > 10 else value
        elif var == "BATCH_TASKS" and value != "NOT_SET":
            value = f"[{len(value)} characters]" if value else "EMPTY"
        print(f"  {var}: {value}")
    print()
    
    if BATCH_MODE:
        print("===================================================")
        print(f"  Starting BATCH Vectorization Job (with Diff Detection)")
        print(f"  Number of tasks: {len(BATCH_TASKS)}")
        print(f"  Features: Auto-add new files + Auto-delete removed files")
        print("===================================================")
        
        total_processed = 0
        total_errors = 0
        
        for i, task in enumerate(BATCH_TASKS, 1):
            uuid = task.get('uuid')
            drive_url = task.get('drive_url')
            company_name = task.get('company_name', '')
            use_embed_v4 = task.get('use_embed_v4', False)
            
            print(f"\nüìã Task {i}/{len(BATCH_TASKS)}: {company_name} (UUID: {uuid})")
            
            try:
                process_single_uuid(uuid, drive_url, use_embed_v4)
                total_processed += 1
                print(f"‚úÖ Task {i} completed successfully")
                    
            except Exception as e:
                print(f"‚ùå Task {i} failed: {e}")
                total_errors += 1
                continue
        
        print(f"\nüéâ Batch job completed: {total_processed} successful, {total_errors} failed")
    else:
        print("===================================================")
        print(f"  Starting SINGLE Vectorization Job (with Diff Detection)")
        print(f"  UUID: {UUID}")
        print(f"  Drive URL: {DRIVE_URL}")
        print(f"  Use Embed V4: {USE_EMBED_V4}")
        print(f"  Features: Auto-add new files + Auto-delete removed files")
        print("===================================================")
        
        all_embeddings = []
        
        def signal_handler(signum, frame):
            print(f"\n‚ö†Ô∏è  Signal {signum} received. Attempting to save current progress...")
            if all_embeddings:
                try:
                    save_checkpoint(GCS_BUCKET_NAME, UUID, all_embeddings, is_final=False)
                    print(f"‚úÖ Emergency save successful: {len(all_embeddings)} embeddings saved")
                except Exception as e:
                    print(f"‚ùå Emergency save failed: {e}")
            sys.exit(1)
        
        signal.signal(signal.SIGTERM, signal_handler)
        signal.signal(signal.SIGINT, signal_handler)
        
        all_embeddings = process_single_uuid(UUID, DRIVE_URL, USE_EMBED_V4)
        print("üéâ Single job finished successfully.")

if __name__ == "__main__":
    main()
