import os
import io
import json
import traceback
import signal
import sys
from datetime import datetime
from typing import Optional, Tuple

import numpy as np
from dotenv import load_dotenv
from google.cloud import storage
from PIL import Image as PILImage

from embedding_providers import get_embedding_provider

# Decompression bombå¯¾ç­–: æœ€å¤§ç”»åƒãƒ”ã‚¯ã‚»ãƒ«æ•°ã‚’è¨­å®šï¼ˆç´„500MPï¼‰
PILImage.MAX_IMAGE_PIXELS = 500_000_000

import google.auth
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseDownload
from drive_scanner import list_files_in_drive_folder

load_dotenv()

# --- 1. ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿ã¨æ¤œè¨¼ ---
BATCH_MODE = os.getenv("BATCH_MODE", "false").lower() == "true"

if BATCH_MODE:
    BATCH_TASKS_JSON = os.getenv("BATCH_TASKS", "[]")
    try:
        BATCH_TASKS = json.loads(BATCH_TASKS_JSON)
    except json.JSONDecodeError:
        raise RuntimeError("FATAL: Invalid BATCH_TASKS JSON format")
else:
    UUID = os.getenv("UUID")
    DRIVE_URL = os.getenv("DRIVE_URL")
    USE_EMBED_V4 = os.getenv("USE_EMBED_V4", "false").lower() == "true"

GCS_BUCKET_NAME = os.getenv("GCS_BUCKET_NAME")
COHERE_API_KEY = os.getenv("COHERE_API_KEY")
GCP_PROJECT_ID = os.getenv("GCP_PROJECT_ID")
GCP_REGION = os.getenv("GCP_REGION", "asia-northeast1")
VERTEX_MULTIMODAL_MODEL = os.getenv("VERTEX_MULTIMODAL_MODEL", "multimodalembedding@001")
EMBEDDING_PROVIDER = os.getenv("EMBEDDING_PROVIDER", "cohere").lower()
MAX_IMAGE_SIZE_MB = 5
CHECKPOINT_INTERVAL = 100

if BATCH_MODE:
    required_vars = ['GCS_BUCKET_NAME', 'GCP_PROJECT_ID']
    missing = [var for var in required_vars if not os.getenv(var)]
    if missing:
        raise RuntimeError(f"FATAL: Required environment variables are missing: {', '.join(missing)}")
    if not BATCH_TASKS:
        raise RuntimeError("FATAL: No tasks provided in batch mode")
else:
    required_vars = ['GCS_BUCKET_NAME', 'GCP_PROJECT_ID', 'UUID', 'DRIVE_URL']
    missing = [var for var in required_vars if not os.getenv(var)]
    if missing:
        raise RuntimeError(f"FATAL: Required environment variables are missing: {', '.join(missing)}")

if EMBEDDING_PROVIDER == "cohere" and not COHERE_API_KEY:
    raise RuntimeError("FATAL: COHERE_API_KEY must be set when EMBEDDING_PROVIDER=cohere")

storage_client = storage.Client()

MAX_FILE_SIZE_BYTES = MAX_IMAGE_SIZE_MB * 1024 * 1024

def resize_image_if_needed(image_content: bytes, filename: str) -> Tuple[Optional[bytes], Optional[str]]:
    """
    ç”»åƒã®è§£åƒåº¦ãŒåŸ‹ã‚è¾¼ã¿APIã®åˆ¶é™ã‚’è¶…ãˆã‚‹å ´åˆã€ãƒ”ã‚¯ã‚»ãƒ«æ•°ãƒ™ãƒ¼ã‚¹ã§ãƒªã‚µã‚¤ã‚ºã™ã‚‹ã€‚
    """
    try:
        try:
            img = PILImage.open(io.BytesIO(image_content))
            img.verify()
            img = PILImage.open(io.BytesIO(image_content))
        except PILImage.DecompressionBombError as e:
            print(f"    âš ï¸  '{filename}' ã§Decompression bombè­¦å‘ŠãŒç™ºç”Ÿ: {e}")
            print("       ç”»åƒãŒæ¥µç«¯ã«å¤§ãã„ã‹ç ´æã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            return None, "decompression_bomb"
        except OSError as e:
            print(f"    âš ï¸  ç”»åƒãƒ•ã‚¡ã‚¤ãƒ« '{filename}' ã‚’åˆ¤åˆ¥ã§ãã¾ã›ã‚“: {e}")
            print("       ç”»åƒã§ãªã„ã‹ç ´æã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            return None, "cannot_identify"
        except Exception as e:
            print(f"    âš ï¸  ç”»åƒ '{filename}' ã®èª­ã¿è¾¼ã¿ä¸­ã«æƒ³å®šå¤–ã®ã‚¨ãƒ©ãƒ¼: {e}")
            return None, "open_error"
            
        original_width, original_height = img.size
        original_pixels = original_width * original_height
        original_size_mb = len(image_content) / (1024 * 1024)
        
        if original_pixels > 100_000_000:
            print(f"    âš ï¸  è¶…é«˜è§£åƒåº¦ç”»åƒã‚’æ¤œå‡º: {original_width}x{original_height} ({original_pixels:,} pixels)")
            print("       å®‰å…¨ã«å‡¦ç†ã§ããªã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            return None, "too_large"
        
        MAX_PIXELS = 2_300_000
        
        if original_pixels <= MAX_PIXELS:
            return image_content, None
        
        print(f"    ğŸ“ é«˜è§£åƒåº¦ç”»åƒã‚’æ¤œå‡º: {original_width}x{original_height} ({original_pixels:,} pixels > {MAX_PIXELS:,})")
        print(f"       ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {original_size_mb:.1f}MB")
        
        if img.mode in ('RGBA', 'LA', 'P'):
            background = PILImage.new('RGB', img.size, (255, 255, 255))
            background.paste(img, mask=img.split()[-1] if 'A' in img.mode else None)
            img = background

        scale_factor = (MAX_PIXELS / original_pixels) ** 0.5
        scale_factor = max(0.3, scale_factor)
        
        new_width = int(original_width * scale_factor)
        new_height = int(original_height * scale_factor)
        new_pixels = new_width * new_height
        
        print(f"    ğŸ”¢ ç¸®å°ã‚¹ã‚±ãƒ¼ãƒ«: {scale_factor:.3f}")
        print(f"       å¤‰æ›å¾Œã®è§£åƒåº¦: {new_width}x{new_height} ({new_pixels:,} pixels)")
        
        resized_img = img.resize((new_width, new_height), PILImage.Resampling.LANCZOS)
        
        output = io.BytesIO()
        resized_img.save(output, format='JPEG', quality=90, optimize=True)
        resized_data = output.getvalue()
        resized_size_mb = len(resized_data) / (1024 * 1024)
        
        quality = 90
        while len(resized_data) > MAX_FILE_SIZE_BYTES and quality >= 60:
            quality -= 10
            output = io.BytesIO()
            resized_img.save(output, format='JPEG', quality=quality, optimize=True)
            resized_data = output.getvalue()
            resized_size_mb = len(resized_data) / (1024 * 1024)
        
        print(f"    âœ… ãƒªã‚µã‚¤ã‚ºå®Œäº†: {original_size_mb:.1f}MB -> {resized_size_mb:.1f}MB")
        print(f"       è§£åƒåº¦: {original_width}x{original_height} -> {new_width}x{new_height}")
        print(f"       å‡ºåŠ›å“è³ª: {quality}")
        
        return resized_data, None
        
    except Exception as e:
        print(f"    âŒ ãƒªã‚µã‚¤ã‚ºä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
        traceback.print_exc()
        return None, "resize_failure"

def get_multimodal_embedding(image_bytes: bytes, filename: str, file_index: int = 0, use_embed_v4: bool = False) -> np.ndarray:
    """ç”»åƒãƒ‡ãƒ¼ã‚¿ã¨ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰é‡ã¿ä»˜ã‘ã•ã‚ŒãŸãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆã™ã‚‹"""
    try:
        provider = get_embedding_provider()
        embedding = provider.embed_multimodal(
            text=filename,
            image_bytes=image_bytes,
            use_embed_v4=use_embed_v4,
        )
        return embedding
    
    except Exception as e:
        print(f"    âš ï¸  '{filename}' ã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã«å¤±æ•—ã—ãŸãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™: {e}")
        traceback.print_exc()
        return None

def load_existing_embeddings(bucket_name: str, uuid: str) -> tuple:
    """æ—¢å­˜ã®embeddingsã¨å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã‚’èª­ã¿è¾¼ã‚€"""
    try:
        bucket = storage_client.bucket(bucket_name)
        blob = bucket.blob(f"{uuid}.json")
        
        if blob.exists():
            existing_data = json.loads(blob.download_as_text())
            processed_files = {item['filename'] for item in existing_data}
            print(f"ğŸ“‚ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ {len(existing_data)} ä»¶èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
            return existing_data, processed_files
        else:
            print("ğŸ“‚ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸãŸã‚æ–°è¦ä½œæˆã—ã¾ã™")
            return [], set()
    except Exception as e:
        print(f"âš ï¸  æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return [], set()

def save_checkpoint(bucket_name: str, uuid: str, embeddings: list, is_final: bool = False):
    """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¨ã—ã¦embeddingsã‚’{uuid}.jsonã«ä¿å­˜"""
    try:
        from datetime import datetime
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        bucket = storage_client.bucket(bucket_name)
        blob = bucket.blob(f"{uuid}.json")
        blob.upload_from_string(
            json.dumps(embeddings, ensure_ascii=False, indent=2),
            content_type="application/json"
        )
        
        if is_final:
            print(f"âœ… [{current_time}] æœ€çµ‚ä¿å­˜å®Œäº†: {len(embeddings)} ä»¶ã‚’ gs://{bucket_name}/{uuid}.json ã«ä¿å­˜ã—ã¾ã—ãŸ")
        else:
            print(f"ğŸ’¾ [{current_time}] ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜: {len(embeddings)} ä»¶ã‚’ gs://{bucket_name}/{uuid}.json ã«é€€é¿ã—ã¾ã—ãŸ")
            
    except Exception as e:
        print(f"âŒ [{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] gs://{bucket_name}/{uuid}.json ã¸ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        traceback.print_exc()

def calculate_diff(drive_files: list, existing_embeddings: list) -> tuple:
    """
    Google Driveä¸Šã®ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã¨æ—¢å­˜ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã¨ã®å·®åˆ†ã‚’ç®—å‡ºã™ã‚‹ã€‚
    
    å¼•æ•°:
        drive_files: Google Driveã‹ã‚‰å–å¾—ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã®ãƒªã‚¹ãƒˆ
        existing_embeddings: æ—¢å­˜ã®ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿
        
    æˆ»ã‚Šå€¤:
        è¿½åŠ å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆã¨ã€å‰Šé™¤å¯¾è±¡ã‚’ç¤ºã™ã‚­ãƒ¼é›†åˆã®ã‚¿ãƒ—ãƒ«
    """
    # Google Driveã®ç¾åœ¨ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚»ãƒƒãƒˆï¼ˆãƒ•ãƒ«ãƒ‘ã‚¹ã§ç®¡ç†ï¼‰
    drive_file_keys = {f"{f.get('folder_path', '')}/{f['name']}" for f in drive_files}
    
    # ãƒ™ã‚¯ãƒˆãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚»ãƒƒãƒˆï¼ˆãƒ•ãƒ«ãƒ‘ã‚¹ã§ç®¡ç†ï¼‰
    vector_file_keys = {f"{item.get('folder_path', '')}/{item.get('filename', '')}" for item in existing_embeddings}
    
    # è¿½åŠ å¯¾è±¡: Driveã«ã‚ã‚‹ãŒãƒ™ã‚¯ãƒˆãƒ«ã«ãªã„
    keys_to_add = drive_file_keys - vector_file_keys
    
    # å‰Šé™¤å¯¾è±¡: ãƒ™ã‚¯ãƒˆãƒ«ã«ã‚ã‚‹ãŒDriveã«ãªã„
    keys_to_delete = vector_file_keys - drive_file_keys
    
    # è¿½åŠ å¯¾è±¡ã®ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã‚’æŠ½å‡º
    files_to_add = [f for f in drive_files if f"{f.get('folder_path', '')}/{f['name']}" in keys_to_add]
    
    print("\nğŸ“Š å·®åˆ†è§£æçµæœ:")
    print(f"   Driveå´ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(drive_file_keys)}")
    print(f"   ãƒ™ã‚¯ãƒˆãƒ«å´ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(vector_file_keys)}")
    print(f"   è¿½åŠ å¯¾è±¡: {len(files_to_add)}")
    print(f"   å‰Šé™¤å¯¾è±¡: {len(keys_to_delete)}")
    
    if keys_to_delete:
        print("\nğŸ—‘ï¸  ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å‰Šé™¤ã•ã‚Œã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§:")
        for key in list(keys_to_delete)[:10]:
            print(f"     - {key}")
        if len(keys_to_delete) > 10:
            print(f"     ... æ®‹ã‚Š {len(keys_to_delete) - 10} ä»¶")
    
    return files_to_add, keys_to_delete

def remove_deleted_files(existing_embeddings: list, keys_to_delete: set) -> list:
    """
    å·®åˆ†è¨ˆç®—ã§åˆ¤å®šã—ãŸå‰Šé™¤å¯¾è±¡ã‚’æ—¢å­˜ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é™¤å¤–ã™ã‚‹ã€‚
    
    å¼•æ•°:
        existing_embeddings: æ—¢å­˜ã®ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿
        keys_to_delete: å‰Šé™¤å¯¾è±¡ã‚’ç¤ºã™ãƒ•ã‚¡ã‚¤ãƒ«ã‚­ãƒ¼é›†åˆ
        
    æˆ»ã‚Šå€¤:
        å‰Šé™¤æ¸ˆã¿ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
    """
    if not keys_to_delete:
        return existing_embeddings.copy()
    
    original_count = len(existing_embeddings)
    
    # å‰Šé™¤å¯¾è±¡ä»¥å¤–ã‚’æ®‹ã™
    filtered_embeddings = [
        item for item in existing_embeddings
        if f"{item.get('folder_path', '')}/{item.get('filename', '')}" not in keys_to_delete
    ]
    
    deleted_count = original_count - len(filtered_embeddings)
    
    print("\nğŸ—‘ï¸  å‰Šé™¤å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ:")
    print(f"   å…ƒãƒ‡ãƒ¼ã‚¿æ•°: {original_count}")
    print(f"   å‰Šé™¤æ•°: {deleted_count}")
    print(f"   æ®‹å­˜æ•°: {len(filtered_embeddings)}")
    
    return filtered_embeddings

def process_single_uuid(uuid: str, drive_url: str, use_embed_v4: bool = False, all_embeddings: list = None) -> list:
    """å˜ä¸€UUIDã®å‡¦ç†ï¼ˆå·®åˆ†æ¤œå‡ºãƒ»å‰Šé™¤æ©Ÿèƒ½ä»˜ãï¼‰"""
    if all_embeddings is None:
        all_embeddings = []
    
    print(f"ğŸ“‹ UUID {uuid} ã®å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™")
    print(f"   Drive URL: {drive_url}")
    print(f"   åˆ©ç”¨ãƒ¢ãƒ‡ãƒ«: {'embed-v4.0' if use_embed_v4 else 'embed-multilingual-v3.0'}")
    
    try:
        # æ—¢å­˜ã®embeddingsã‚’èª­ã¿è¾¼ã‚€
        existing_embeddings, _ = load_existing_embeddings(GCS_BUCKET_NAME, uuid)
        drive_files = list_files_in_drive_folder(drive_url)
        if not drive_files:
            print(f"âš ï¸  Google Driveã«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: UUID {uuid}")
            if existing_embeddings:
                print(f"ğŸ—‘ï¸  DriveãŒç©ºã®ãŸã‚ {len(existing_embeddings)} ä»¶ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’å‰Šé™¤ã—ã¾ã™")
                save_checkpoint(GCS_BUCKET_NAME, uuid, [], is_final=True)
            return []
        
        # å·®åˆ†ã‚’è¨ˆç®—
        files_to_add, keys_to_delete = calculate_diff(drive_files, existing_embeddings)
        
        # å‰Šé™¤å‡¦ç†ã‚’å®Ÿè¡Œ
        task_embeddings = remove_deleted_files(existing_embeddings, keys_to_delete)
        
        # å‰Šé™¤ãŒç™ºç”Ÿã—ãŸå ´åˆã¯å³åº§ã«ä¿å­˜
        if keys_to_delete:
            save_checkpoint(GCS_BUCKET_NAME, uuid, task_embeddings, is_final=False)
            print(f"ğŸ’¾ å‰Šé™¤å¾Œã®ä¸­é–“ä¿å­˜ã‚’å®Ÿæ–½: {len(task_embeddings)} ä»¶")
        
        # è¿½åŠ å¯¾è±¡ãŒãªã„å ´åˆã¯çµ‚äº†
        if not files_to_add:
            print(f"âœ… æ–°è¦å‡¦ç†å¯¾è±¡ã¯ã‚ã‚Šã¾ã›ã‚“ (UUID {uuid})")
            if keys_to_delete:
                # å‰Šé™¤ã®ã¿ç™ºç”Ÿã—ãŸå ´åˆã¯æœ€çµ‚ä¿å­˜
                save_checkpoint(GCS_BUCKET_NAME, uuid, task_embeddings, is_final=True)
            return task_embeddings
        
        print(f"\nğŸ“ æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ« {len(files_to_add)} ä»¶ã®å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...")
        
        print("Google Driveã‚µãƒ¼ãƒ“ã‚¹ã‚’åˆæœŸåŒ–ã—ã¦ã„ã¾ã™...")
        drive_creds, _ = google.auth.default(scopes=['https://www.googleapis.com/auth/drive.readonly'])
        drive_service = build('drive', 'v3', credentials=drive_creds)
        
        start_time = datetime.now()
        
        for i, file_info in enumerate(files_to_add, 1):
            print(f"    ({i}/{len(files_to_add)}) å‡¦ç†ä¸­: {file_info['name'][:50]}...")
            
            try:
                request = drive_service.files().get_media(fileId=file_info['id'])
                fh = io.BytesIO()
                downloader = MediaIoBaseDownload(fh, request)
                done = False
                while not done:
                    _, done = downloader.next_chunk()
                image_content = fh.getvalue()
                
                resized_content, resize_error = resize_image_if_needed(image_content, file_info['name'])
                if resized_content is None:
                    reason_text = resize_error or "unknown_error"
                    print(f"      â­•ï¸  ãƒªã‚µã‚¤ã‚ºã§ããªã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ ({reason_text})")
                    error_entry = {
                        "filename": file_info['name'],
                        "filepath": file_info.get('webViewLink'),
                        "folder_path": file_info.get('folder_path'),
                        "embedding": None,
                        "is_corrupt": True,
                        "corrupt_reason": reason_text,
                    }
                    task_embeddings.append(error_entry)
                    continue

                embedding = get_multimodal_embedding(resized_content, file_info['name'], i, use_embed_v4)
                if embedding is not None:
                    result_data = {
                        "filename": file_info['name'],
                        "filepath": file_info['webViewLink'],
                        "folder_path": file_info['folder_path'],
                        "embedding": embedding.tolist(),
                        "is_corrupt": False,
                    }
                    task_embeddings.append(result_data)
                    
                    if i % CHECKPOINT_INTERVAL == 0:
                        print(f"ğŸ“Œ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ: {len(files_to_add)} ä»¶ä¸­ {i} ä»¶å‡¦ç†æ¸ˆã¿")
                        save_checkpoint(GCS_BUCKET_NAME, uuid, task_embeddings, is_final=False)
                        print(f"ğŸ’¾ ç¾åœ¨ã®åŸ‹ã‚è¾¼ã¿æ•°: {len(task_embeddings)} ä»¶")

            except Exception as e:
                print(f"      âŒ {file_info['name']} ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        # ã‚¿ã‚¹ã‚¯å®Œäº†å¾Œã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
        if task_embeddings != existing_embeddings or keys_to_delete:
            elapsed_total = (datetime.now() - start_time).total_seconds()
            print(f"   â±ï¸  UUID {uuid} ã®å‡¦ç†æ™‚é–“: {elapsed_total:.1f} ç§’")
            save_checkpoint(GCS_BUCKET_NAME, uuid, task_embeddings, is_final=True)
            print(f"   âœ… UUID {uuid} ç”¨ã« {len(task_embeddings)} ä»¶ä¿å­˜ã—ã¾ã—ãŸ")
            print(f"   ğŸ“Š å¤‰åŒ–é‡: è¿½åŠ  {len(files_to_add)} ä»¶ / å‰Šé™¤ {len(keys_to_delete)} ä»¶")
        
        return task_embeddings
        
    except Exception as e:
        print(f"   âŒ UUID {uuid} ã®å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
        traceback.print_exc()
        if task_embeddings:
            try:
                save_checkpoint(GCS_BUCKET_NAME, uuid, task_embeddings, is_final=False)
                print(f"   ğŸ’¾ UUID {uuid} ã®é€”ä¸­çµæœã‚’ç·Šæ€¥ä¿å­˜ ({len(task_embeddings)} ä»¶)")
            except Exception as save_error:
                print(f"   âŒ UUID {uuid} ã®ç·Šæ€¥ä¿å­˜ã«å¤±æ•—: {save_error}")
        raise e


def main():
    """Cloud Runã‚¸ãƒ§ãƒ–ã¨ã—ã¦å®Ÿè¡Œã•ã‚Œã‚‹ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    
    print("ğŸ”§ ä½¿ç”¨ç’°å¢ƒå¤‰æ•°ä¸€è¦§:")
    env_vars = [
        "GCS_BUCKET_NAME", "GCP_PROJECT_ID", "GCP_REGION", "VERTEX_MULTIMODAL_MODEL",
        "EMBEDDING_PROVIDER", "COHERE_API_KEY",
        "UUID", "DRIVE_URL", "USE_EMBED_V4", "BATCH_MODE", "BATCH_TASKS"
    ]
    for var in env_vars:
        value = os.getenv(var, "NOT_SET")
        if var == "COHERE_API_KEY" and value != "NOT_SET":
            value = f"{value[:10]}..." if len(value) > 10 else value
        elif var == "BATCH_TASKS" and value != "NOT_SET":
            value = f"[{len(value)} characters]" if value else "EMPTY"
        print(f"  {var}: {value}")
    print()
    
    if BATCH_MODE:
        print("===================================================")
        print("  ãƒãƒƒãƒãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚¸ãƒ§ãƒ–ï¼ˆå·®åˆ†æ¤œå‡ºã‚ã‚Šï¼‰ã‚’é–‹å§‹ã—ã¾ã™")
        print(f"  ã‚¿ã‚¹ã‚¯æ•°: {len(BATCH_TASKS)}")
        print("  æ©Ÿèƒ½: æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«ã®è‡ªå‹•è¿½åŠ  + å‰Šé™¤ãƒ•ã‚¡ã‚¤ãƒ«ã®è‡ªå‹•é™¤å»")
        print("===================================================")
        
        total_processed = 0
        total_errors = 0
        
        for i, task in enumerate(BATCH_TASKS, 1):
            uuid = task.get('uuid')
            drive_url = task.get('drive_url')
            company_name = task.get('company_name', '')
            use_embed_v4 = task.get('use_embed_v4', False)
            
            print(f"\nğŸ“‹ ã‚¿ã‚¹ã‚¯ {i}/{len(BATCH_TASKS)}: {company_name} (UUID: {uuid})")
            
            try:
                process_single_uuid(uuid, drive_url, use_embed_v4)
                total_processed += 1
                print(f"âœ… ã‚¿ã‚¹ã‚¯ {i}ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ")
                    
            except Exception as e:
                print(f"âŒ ã‚¿ã‚¹ã‚¯ {i}ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                total_errors += 1
                continue
        
        print(f"\nğŸ‰ ãƒãƒƒãƒå‡¦ç†å®Œäº†: æˆåŠŸ {total_processed} ä»¶ / å¤±æ•— {total_errors} ä»¶")
    else:
        print("===================================================")
        print("  å˜ä½“ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚¸ãƒ§ãƒ–ï¼ˆå·®åˆ†æ¤œå‡ºã‚ã‚Šï¼‰ã‚’é–‹å§‹ã—ã¾ã™")
        print(f"  UUID: {UUID}")
        print(f"  Drive URL: {DRIVE_URL}")
        print(f"  Embed V4åˆ©ç”¨: {USE_EMBED_V4}")
        print("  æ©Ÿèƒ½: æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«ã®è‡ªå‹•è¿½åŠ  + å‰Šé™¤ãƒ•ã‚¡ã‚¤ãƒ«ã®è‡ªå‹•é™¤å»")
        print("===================================================")
        
        all_embeddings = []
        
        def signal_handler(signum, frame):
            print(f"\nâš ï¸  ã‚·ã‚°ãƒŠãƒ« {signum} ã‚’å—ä¿¡ã—ãŸãŸã‚ã€ä¸­é–“çµæœã‚’ä¿å­˜ã—ã¾ã™...")
            if all_embeddings:
                try:
                    save_checkpoint(GCS_BUCKET_NAME, UUID, all_embeddings, is_final=False)
                    print(f"âœ… ç·Šæ€¥ä¿å­˜ãŒå®Œäº†ã—ã¾ã—ãŸ: {len(all_embeddings)} ä»¶")
                except Exception as e:
                    print(f"âŒ ç·Šæ€¥ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            sys.exit(1)
        
        signal.signal(signal.SIGTERM, signal_handler)
        signal.signal(signal.SIGINT, signal_handler)
        
        all_embeddings = process_single_uuid(UUID, DRIVE_URL, USE_EMBED_V4)
        print("ğŸ‰ å˜ä½“ã‚¸ãƒ§ãƒ–ãŒæ­£å¸¸ã«çµ‚äº†ã—ã¾ã—ãŸã€‚")

if __name__ == "__main__":
    main()
