import os
import io
import json
import traceback
import base64
import hashlib

import cohere
import numpy as np
from dotenv import load_dotenv
from google.cloud import storage
from PIL import Image

import google.auth
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseDownload
from drive_scanner import list_files_in_drive_folder # drive_scanner.pyã‚’å†åˆ©ç”¨

load_dotenv()

# --- 1. ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿ã¨æ¤œè¨¼ ---
UUID = os.getenv("UUID")
DRIVE_URL = os.getenv("DRIVE_URL")
GCS_BUCKET_NAME = os.getenv("GCS_BUCKET_NAME")
COHERE_API_KEY = os.getenv("COHERE_API_KEY")
MAX_IMAGE_SIZE_MB = 5  # Cohere APIåˆ¶é™: æœ€å¤§5MB

if not all([GCS_BUCKET_NAME, COHERE_API_KEY, UUID, DRIVE_URL]):
    missing = [
        var for var in ['GCS_BUCKET_NAME', 'COHERE_API_KEY', 'UUID', 'DRIVE_URL']
        if not os.getenv(var)
    ]
    raise RuntimeError(f"FATAL: Required environment variables are missing: {', '.join(missing)}")

# --- 2. ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ– ---
co_client = cohere.Client(COHERE_API_KEY)
storage_client = storage.Client()
MAX_FILE_SIZE_BYTES = MAX_IMAGE_SIZE_MB * 1024 * 1024

def resize_image_if_needed(image_content: bytes, filename: str) -> bytes:
    """
    ç”»åƒã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒä¸Šé™ã‚’è¶…ãˆã‚‹å ´åˆã€ã‚µã‚¤ã‚ºã‹ã‚‰å¿…è¦ãªãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é‡ã‚’è¨ˆç®—ã—ã¦ãƒªã‚µã‚¤ã‚ºã™ã‚‹ã€‚
    """
    if len(image_content) <= MAX_FILE_SIZE_BYTES:
        return image_content

    original_size_mb = len(image_content) / (1024 * 1024)
    print(f"    ğŸ“ Large file detected ({original_size_mb:.1f}MB > 5MB limit): {filename}. Calculating optimal resize...")
    
    try:
        img = Image.open(io.BytesIO(image_content))
        original_width, original_height = img.size
        
        # RGBAã‚„Pãƒ¢ãƒ¼ãƒ‰ã®ç”»åƒã‚’RGBã«å¤‰æ›
        if img.mode in ('RGBA', 'LA', 'P'):
            background = Image.new('RGB', img.size, (255, 255, 255))
            background.paste(img, mask=img.split()[-1] if 'A' in img.mode else None)
            img = background

        # ç›®æ¨™ã‚µã‚¤ã‚ºã‚’4.8MBã«è¨­å®šï¼ˆä½™è£•ã‚’æŒãŸã›ã‚‹ï¼‰
        target_size_bytes = int(MAX_FILE_SIZE_BYTES * 0.96)  # 4.8MB
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã®åœ§ç¸®ç‡ã‚’è¨ˆç®—ï¼ˆç·šå½¢è¿‘ä¼¼ï¼‰
        # ç”»åƒã®ãƒ”ã‚¯ã‚»ãƒ«æ•°ã¯ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã«ã»ã¼æ¯”ä¾‹ã™ã‚‹ã¨ä»®å®š
        size_ratio = target_size_bytes / len(image_content)
        
        # å¿…è¦ãªè§£åƒåº¦ã‚¹ã‚±ãƒ¼ãƒ«ã‚’è¨ˆç®—ï¼ˆé¢ç©æ¯”ã®å¹³æ–¹æ ¹ï¼‰
        scale_factor = min(1.0, (size_ratio ** 0.5))
        
        # æœ€å°ã§ã‚‚0.3å€ã¾ã§ã—ã‹ã‚¹ã‚±ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³ã—ãªã„ï¼ˆå“è³ªä¿æŒã®ãŸã‚ï¼‰
        scale_factor = max(0.3, scale_factor)
        
        new_width = int(original_width * scale_factor)
        new_height = int(original_height * scale_factor)
        
        print(f"    ğŸ”¢ Calculated scale factor: {scale_factor:.3f} ({original_width}x{original_height} -> {new_width}x{new_height})")
        
        # è¨ˆç®—ã•ã‚ŒãŸã‚µã‚¤ã‚ºã§ãƒªã‚µã‚¤ã‚º
        resized_img = img.resize((new_width, new_height), Image.Resampling.LANCZOS)
        
        # ã¾ãšå“è³ª90ã§è©¦ã™
        output = io.BytesIO()
        resized_img.save(output, format='JPEG', quality=90, optimize=True)
        resized_data = output.getvalue()
        
        # ã¾ã å¤§ãã„å ´åˆã¯å“è³ªã‚’æ®µéšçš„ã«ä¸‹ã’ã‚‹
        quality = 90
        while len(resized_data) > MAX_FILE_SIZE_BYTES and quality >= 50:
            quality -= 10
            output = io.BytesIO()
            resized_img.save(output, format='JPEG', quality=quality, optimize=True)
            resized_data = output.getvalue()
            
        final_size_mb = len(resized_data) / (1024 * 1024)
        
        if len(resized_data) <= MAX_FILE_SIZE_BYTES:
            print(f"    âœ… Successfully resized: {original_size_mb:.1f}MB -> {final_size_mb:.1f}MB")
            print(f"       Scale: {scale_factor:.1%}, Quality: {quality}, Size: {new_width}x{new_height}")
            return resized_data
        else:
            # ãã‚Œã§ã‚‚å¤§ãã„å ´åˆã¯ã€ã•ã‚‰ã«è§£åƒåº¦ã‚’ä¸‹ã’ã‚‹
            print(f"    ğŸ”„ Still too large ({final_size_mb:.1f}MB), applying additional scaling...")
            
            # ã‚ˆã‚Šå¼·ã„ã‚¹ã‚±ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³ã‚’é©ç”¨
            additional_scale = 0.8
            final_width = int(new_width * additional_scale)
            final_height = int(new_height * additional_scale)
            
            final_img = resized_img.resize((final_width, final_height), Image.Resampling.LANCZOS)
            output = io.BytesIO()
            final_img.save(output, format='JPEG', quality=60, optimize=True)
            final_data = output.getvalue()
            
            final_size_mb = len(final_data) / (1024 * 1024)
            
            if len(final_data) <= MAX_FILE_SIZE_BYTES:
                print(f"    âœ… Final resize successful: {original_size_mb:.1f}MB -> {final_size_mb:.1f}MB")
                print(f"       Final size: {final_width}x{final_height}, Quality: 60")
                return final_data
            else:
                print(f"    âš ï¸ Warning: Could not resize {filename} below 5MB limit. Final size: {final_size_mb:.1f}MB. Skipping.")
                return None
        
    except Exception as e:
        print(f"    âŒ Resize Error for {filename}: {e}")
        traceback.print_exc()
        return None

def get_multimodal_embedding(image_bytes: bytes, filename: str) -> np.ndarray:
    """ç”»åƒãƒ‡ãƒ¼ã‚¿ã¨ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰é‡ã¿ä»˜ã‘ã•ã‚ŒãŸãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆã™ã‚‹"""
    try:
        # 1. ãƒ•ã‚¡ã‚¤ãƒ«åã‚’textã¨ã—ã¦ãƒ™ã‚¯ãƒˆãƒ«åŒ–
        text_response = co_client.embed(
            texts=[filename],
            model="embed-multilingual-v3.0",
            input_type="search_document"
        )
        text_vec = np.array(text_response.embeddings[0])
        
        # 2. ç”»åƒã‚’imageã¨ã—ã¦ãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼ˆdata URIå½¢å¼ã§é€ä¿¡ï¼‰
        file_extension = filename.lower().split('.')[-1]
        if file_extension in ['jpg', 'jpeg']:
            mime_type = 'jpeg'
        elif file_extension in ['png']:
            mime_type = 'png'
        elif file_extension in ['gif']:
            mime_type = 'gif'
        elif file_extension in ['webp']:
            mime_type = 'webp'
        else:
            mime_type = 'jpeg'
        
        base64_string = base64.b64encode(image_bytes).decode("utf-8")
        data_uri = f"data:image/{mime_type};base64,{base64_string}"
        
        image_response = co_client.embed(
            images=[data_uri],
            model="embed-multilingual-v3.0",
            input_type="image"
        )
        image_vec = np.array(image_response.embeddings[0])
        
        # 3. ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦wã‚’è¨ˆç®—
        dot_product = np.dot(text_vec, image_vec)
        norm_text = np.linalg.norm(text_vec)
        norm_image = np.linalg.norm(image_vec)
        w = dot_product / (norm_text * norm_image)
        
        # wã‚’0-1ã®ç¯„å›²ã«ã‚¯ãƒªãƒƒãƒ—ï¼ˆè² ã®å€¤ã‚’é¿ã‘ã‚‹ï¼‰
        w = max(0, min(1, w))
        
        # 4. é‡ã¿ä»˜ã‘çµ±åˆãƒ™ã‚¯ãƒˆãƒ«ã‚’è¨ˆç®—
        final_vec = w * text_vec + (1 - w) * image_vec
        
        print(f"    ğŸ“Š Text-Image similarity: {w:.3f} for '{filename}'")
        return final_vec
        
    except Exception as e:
        print(f"    âš ï¸  Warning: Could not generate multimodal embedding for '{filename}'. Skipping. Reason: {e}")
        return None

def main():
    """Cloud Runã‚¸ãƒ§ãƒ–ã¨ã—ã¦å®Ÿè¡Œã•ã‚Œã‚‹ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print("===================================================")
    print(f"  Starting Vectorization Job for UUID: {UUID}")
    print(f"  Target Drive URL: {DRIVE_URL}")
    print("===================================================")

    try:
        files_to_process = list_files_in_drive_folder(DRIVE_URL)
        if not files_to_process:
            print("âœ… No processable images found. Job finished successfully.")
            return

        print(f"Found {len(files_to_process)} images. Initializing Google Drive service...")
        
        all_embeddings = []
        drive_creds, _ = google.auth.default(scopes=['https://www.googleapis.com/auth/drive.readonly'])
        drive_service = build('drive', 'v3', credentials=drive_creds)

        for i, file_info in enumerate(files_to_process, 1):
            print(f"  ({i}/{len(files_to_process)}) Processing: {file_info['folder_path']}/{file_info['name']}...")
            try:
                # 1. Download image from Google Drive
                request = drive_service.files().get_media(fileId=file_info['id'])
                fh = io.BytesIO()
                downloader = MediaIoBaseDownload(fh, request)
                done = False
                while not done:
                    _, done = downloader.next_chunk()
                image_content = fh.getvalue()
                
                # 2. Resize if necessary
                resized_content = resize_image_if_needed(image_content, file_info['name'])
                if resized_content is None:
                    continue

                # 3. Get multimodal embedding
                embedding = get_multimodal_embedding(resized_content, file_info['name'])
                if embedding is not None:
                    result_data = {
                        "filename": file_info['name'],
                        "filepath": file_info['webViewLink'],
                        "folder_path": file_info['folder_path'],
                        "embedding": embedding.tolist()
                    }
                    all_embeddings.append(result_data)

            except Exception as e:
                print(f"    -> Error processing file {file_info['name']}: {e}")
        
        if not all_embeddings:
            print("âš ï¸  No embeddings were generated. Check for previous warnings.")
            return
            
        print(f"\nGenerated {len(all_embeddings)} embeddings. Uploading to Cloud Storage...")
        
        bucket = storage_client.bucket(GCS_BUCKET_NAME)
        blob = bucket.blob(f"{UUID}.json")
        blob.upload_from_string(
            json.dumps(all_embeddings, ensure_ascii=False, indent=2),
            content_type="application/json"
        )
        
        print(f"âœ… Successfully saved vector data to gs://{GCS_BUCKET_NAME}/{UUID}.json")
        print("ğŸ‰ Job finished successfully.")

    except Exception as e:
        print(f"âŒ An unexpected error occurred during the job execution:")
        traceback.print_exc()
        raise e

if __name__ == "__main__":
    main()

