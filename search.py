"""
Google Cloud Storageã‹ã‚‰ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€é¡ä¼¼æ¤œç´¢ã‚„ãƒ©ãƒ³ãƒ€ãƒ æ¤œç´¢ã‚’è¡Œã†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã€‚
"""

import os
import json
import traceback
from typing import List, Dict, Optional

import numpy as np
from google.cloud import storage


class StorageClient:
    """ç’°å¢ƒã«å¿œã˜ã¦Google Cloud Storageã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–ã™ã‚‹ãƒ©ãƒƒãƒ‘ãƒ¼ã€‚"""
    
    def __init__(self):
        self._client = self._get_storage_client()
    
    def _get_storage_client(self) -> storage.Client:
        """
        å®Ÿè¡Œç’°å¢ƒã«å¿œã˜ãŸèªè¨¼æ–¹æ³•ã§GCSã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ç”Ÿæˆã™ã‚‹ã€‚
        
        æˆ»ã‚Šå€¤:
            storage.Client: åˆæœŸåŒ–æ¸ˆã¿ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        """
        environment = os.getenv("ENVIRONMENT", "local")
        # ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºæ™‚ã®ã¿ã“ã®éµãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ã€æœ¬ç•ªã§ã¯Cloud Runã®ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚’åˆ©ç”¨ã™ã‚‹ã€‚
        key_file = "marketing-automation-461305-2acf4965e0b0.json"

        if environment == "production":
            print("ğŸŒ Production environment: Initializing GCS client with default credentials.")
            return storage.Client()
        else:
            print(f"ğŸ  Local environment: Looking for '{key_file}'...")
            if os.path.exists(key_file):
                print(f"   âœ… Using key file '{key_file}'.")
                return storage.Client.from_service_account_json(key_file)
            else:
                print(f"   âš ï¸ Key file not found. Falling back to default credentials.")
                return storage.Client()
    
    @property
    def client(self) -> storage.Client:
        """ç”Ÿæˆæ¸ˆã¿ã®Storageã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’è¿”ã™ã€‚"""
        return self._client


class ImageSearcher:
    """
    ä¼æ¥­ã”ã¨ã®ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€æ¤œç´¢å‡¦ç†ã‚’æä¾›ã™ã‚‹ã‚¯ãƒ©ã‚¹ã€‚
    """
    
    def __init__(self, uuid: str, bucket_name: Optional[str] = None, model_name: Optional[str] = None):
        """
        æŒ‡å®šã—ãŸUUIDå‘ã‘ã«æ¤œç´¢ã‚’è¡Œã†ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’åˆæœŸåŒ–ã™ã‚‹ã€‚
        
        å¼•æ•°:
            uuid: ä¼æ¥­ã®UUID
            bucket_name: ãƒ™ã‚¯ãƒˆãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ ¼ç´ã—ã¦ã„ã‚‹GCSãƒã‚±ãƒƒãƒˆ
            model_name: å‚ç…§ã™ã‚‹ãƒ¢ãƒ‡ãƒ«è­˜åˆ¥å­ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            
        ä¾‹å¤–:
            ValueError: bucket_nameãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆ
            FileNotFoundError: å¯¾å¿œã™ã‚‹ãƒ™ã‚¯ãƒˆãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆ
        """
        if not bucket_name:
            raise ValueError("GCS bucket name is not provided.")
            
        self.uuid = uuid
        self.bucket_name = bucket_name
        self.model_name = (model_name or "").strip().lower() or None
        self.embeddings_data: List[Dict] = []
        self.embeddings_matrix: Optional[np.ndarray] = None
        self.storage_client = StorageClient()
        self._loaded_blob_path: Optional[str] = None
        self.total_entries_count: int = 0
        self.corrupt_entries_count: int = 0
        self.invalid_entries_count: int = 0
        
        self._load_data()

    def _candidate_blob_paths(self) -> List[str]:
        """
        ç¾çŠ¶ã®é‹ç”¨ã§ã¯UUIDã”ã¨ã«å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ{uuid}.jsonï¼‰ã®ã¿ã‚’æœŸå¾…ã™ã‚‹ã€‚
        å°†æ¥çš„ã«ãƒ¢ãƒ‡ãƒ«åˆ¥ãƒ‘ã‚¹ã«æ‹¡å¼µã™ã‚‹å ´åˆã¯ã“ã“ã§åˆ†å²ã‚’è¿½åŠ ã™ã‚‹ã€‚
        """
        return [f"{self.uuid}.json"]

    def _load_data(self) -> None:
        """
        GCSä¸Šã®JSONãƒ™ã‚¯ãƒˆãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ãƒ¡ãƒ¢ãƒªã«ä¿æŒã™ã‚‹ã€‚
        
        ä¾‹å¤–:
            FileNotFoundError: ãƒ™ã‚¯ãƒˆãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆ
            Exception: èª­ã¿è¾¼ã¿ã¾ãŸã¯ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ãŸå ´åˆ
        """
        bucket = self.storage_client.client.bucket(self.bucket_name)
        blob = None
        file_path = None

        candidates = self._candidate_blob_paths()
        if self.model_name:
            print(f"   ğŸ” Requested model hint: {self.model_name}")

        for candidate in candidates:
            candidate_blob = bucket.blob(candidate)
            if candidate_blob.exists():
                blob = candidate_blob
                file_path = candidate
                break

        if blob is None or file_path is None:
            attempted = ", ".join(candidates)
            print(f"âŒ ERROR: Vector file not found for UUID '{self.uuid}'. Tried: {attempted}")
            raise FileNotFoundError(f"Vector data for UUID '{self.uuid}' not found.")

        self._loaded_blob_path = file_path
        print(f"ğŸ” Loading vector data for UUID '{self.uuid}' from gs://{self.bucket_name}/{file_path}")
        print(f"   ğŸ“ Vector source: {file_path}")

        try:
            json_data = blob.download_as_string()
            raw_data = json.loads(json_data)

            if not isinstance(raw_data, list):
                raise ValueError("Vector file format is invalid. Expected a list of entries.")

            self.total_entries_count = len(raw_data)
            self.corrupt_entries_count = 0
            self.invalid_entries_count = 0

            filtered_items: List[Dict] = []
            embeddings_list: List[List[float]] = []

            for item in raw_data:
                if item.get("is_corrupt"):
                    self.corrupt_entries_count += 1
                    continue
                embedding = item.get("embedding")
                if not embedding:
                    self.invalid_entries_count += 1
                    continue
                filtered_items.append(item)
                embeddings_list.append(embedding)

            self.embeddings_data = filtered_items

            if embeddings_list:
                # Create a NumPy matrix from the embeddings for efficient calculation
                self.embeddings_matrix = np.array(embeddings_list, dtype=np.float32)
                print(f"âœ… Successfully loaded and processed {len(self.embeddings_data)} vectors.")
            else:
                self.embeddings_matrix = np.array([], dtype=np.float32)
                print("âš ï¸  Warning: No valid embeddings available after filtering.")

            if self.corrupt_entries_count:
                print(f"   âš ï¸ Skipped {self.corrupt_entries_count} entries marked as corrupt.")
            if self.invalid_entries_count:
                print(f"   âš ï¸ Skipped {self.invalid_entries_count} entries without embeddings.")
            if self.total_entries_count and not self.corrupt_entries_count and not self.invalid_entries_count:
                print(f"   â„¹ï¸  Total entries loaded: {self.total_entries_count}")

        except FileNotFoundError:
            raise
        except Exception as e:
            print(f"âŒ Failed to load or parse data for UUID {self.uuid}: {e}")
            traceback.print_exc()
            raise Exception(f"Failed to load vector data for UUID {self.uuid}") from e
            
    def search_images(self, query_embedding: np.ndarray, top_k: int, exclude_files: Optional[List[str]] = None, top_n_pool: int = 25) -> List[Dict]:
        """
        ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã§ä¸Šä½å€™è£œã‚’å–å¾—ã—ã€ãã®ä¸­ã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ æŠ½å‡ºã§top_kä»¶ã‚’è¿”ã™ã€‚
        
        å¼•æ•°:
            query_embedding: æ¤œç´¢ã‚¯ã‚¨ãƒªã®ãƒ™ã‚¯ãƒˆãƒ«
            top_k: è¿”å´ä»¶æ•°
            exclude_files: å€™è£œã‹ã‚‰é™¤å¤–ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«åãƒªã‚¹ãƒˆ
            top_n_pool: ãƒ©ãƒ³ãƒ€ãƒ æŠ½å‡ºã®æ¯æ•°ã¨ãªã‚‹ä¸Šä½å€™è£œæ•°
            
        æˆ»ã‚Šå€¤:
            é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢ä»˜ãã®çµæœè¾æ›¸ãƒªã‚¹ãƒˆ
        """
        top_n_pool = top_k
        if self.embeddings_matrix is None or len(self.embeddings_matrix) == 0:
            print("âš ï¸ No embeddings data available for search")
            return []

        print(f"ğŸ” Performing similarity search with random selection (pool={top_n_pool}, select={top_k})")
        
        # Convert exclude_files to a set for faster lookup
        exclude_set = set(exclude_files) if exclude_files else set()
        
        try:
            # Filter embeddings data to exclude specified files BEFORE similarity calculation
            valid_indices = []
            excluded_count = 0
            
            for i, item in enumerate(self.embeddings_data):
                filename = item.get("filename")
                if filename in exclude_set:
                    excluded_count += 1
                    print(f"   Excluding from search candidates: {filename}")
                else:
                    valid_indices.append(i)
            
            if not valid_indices:
                print("âš ï¸ No search candidates available after applying exclusion list")
                return []
            
            print(f"   Search candidates: {len(valid_indices)} (excluded {excluded_count} files)")
            
            # Create filtered embeddings matrix from valid candidates only
            filtered_embeddings = self.embeddings_matrix[valid_indices]
            
            # Calculate cosine similarity only for valid candidates
            similarities = np.dot(filtered_embeddings, query_embedding) / (
                np.linalg.norm(filtered_embeddings, axis=1) * np.linalg.norm(query_embedding)
            )
            
            # Get top-n indices sorted by similarity (descending) for the pool
            pool_size = min(top_n_pool, len(similarities))
            top_pool_indices = np.argsort(similarities)[::-1][:pool_size]
            
            # Randomly select top_k items from the pool
            num_results = min(top_k, len(top_pool_indices))
            selected_pool_indices = np.random.choice(len(top_pool_indices), num_results, replace=False)
            selected_indices = top_pool_indices[selected_pool_indices]
            
            results = []
            for idx in selected_indices:
                # Map back to original embeddings_data index
                original_idx = valid_indices[idx]
                result = {
                    "filename": self.embeddings_data[original_idx].get("filename"),
                    "filepath": self.embeddings_data[original_idx].get("filepath"),
                    "similarity": float(similarities[idx])
                }
                results.append(result)
            
            # Sort results by similarity for better output readability
            results.sort(key=lambda x: x['similarity'], reverse=True)
            
            print(f"âœ… Randomly selected {len(results)} images from top {pool_size} similar candidates")
            if results:
                print(f"   Similarity range: {results[0]['similarity']:.4f} ~ {results[-1]['similarity']:.4f}")
                
            return results
            
        except Exception as e:
            print(f"âŒ Error during similarity search: {e}")
            traceback.print_exc()
            return []

    def random_image_search(self, count: int, exclude_files: Optional[List[str]] = None) -> List[Dict]:
        """
        ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã«ç”»åƒã‚’é¸æŠã™ã‚‹ã€‚
        
        å¼•æ•°:
            count: è¿”å´ã™ã‚‹ä»¶æ•°
            exclude_files: é™¤å¤–ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«åãƒªã‚¹ãƒˆ
            
        æˆ»ã‚Šå€¤:
            ãƒ©ãƒ³ãƒ€ãƒ ã«æŠ½å‡ºã—ãŸçµæœè¾æ›¸ãƒªã‚¹ãƒˆ
        """
        if not self.embeddings_data:
            print("âš ï¸ No embeddings data available for random search")
            return []
        
        print(f"ğŸ² Performing random search for count={count}")
        
        # Convert exclude_files to a set for faster lookup
        exclude_set = set(exclude_files) if exclude_files else set()
        if exclude_set:
            print(f"   Excluding {len(exclude_set)} files from random selection")
        
        try:
            # Filter out excluded files first
            valid_indices = []
            for i, item in enumerate(self.embeddings_data):
                filename = item.get("filename")
                if filename not in exclude_set:
                    valid_indices.append(i)
                else:
                    print(f"   Excluding from pool: {filename}")
            
            if not valid_indices:
                print("âš ï¸ No images available after applying exclusion list")
                return []
            
            # Sample from valid indices only
            num_to_sample = min(count, len(valid_indices))
            selected_indices = np.random.choice(valid_indices, num_to_sample, replace=False)
            
            results = []
            for i in selected_indices:
                result = {
                    "filename": self.embeddings_data[i].get("filename"),
                    "filepath": self.embeddings_data[i].get("filepath"),
                    "similarity": None  # No similarity score for random search
                }
                results.append(result)
            
            excluded_count = len(self.embeddings_data) - len(valid_indices)
            print(f"âœ… Selected {len(results)} random images from {len(valid_indices)} available (excluded {excluded_count})")
            return results
            
        except Exception as e:
            print(f"âŒ Error during random search: {e}")
            traceback.print_exc()
            return []
