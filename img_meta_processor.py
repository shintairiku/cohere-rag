"""
ç”»åƒå‡¦ç†å°‚ç”¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆ - RAGãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰
"""

import os
import io
import base64
import json
import hashlib
from pathlib import Path
from typing import List, Dict, Set
from dotenv import load_dotenv
import cohere
import numpy as np
from PIL import Image

load_dotenv()

class ImageProcessor:
    def __init__(self, images_dir: str = "images", embeddings_file: str = "embeddings.json"):
        self.images_dir = Path(images_dir)
        self.embeddings_file = embeddings_file
        # ğŸ†• è¿½åŠ : Cohere APIã®åˆ¶é™ï¼ˆ20MBï¼‰
        self.max_file_size = 20 * 1024 * 1024  # 20MB in bytes
        self.api_key = os.getenv("COHERE_API_KEY")
        
        if not self.api_key:
            raise ValueError("COHERE_API_KEY not found in environment variables")
        
        self.client = cohere.ClientV2(api_key=self.api_key)
        self.processed_images: Set[str] = set()
        self.embeddings_data: List[Dict] = []
        
        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        self.load_existing_data()
    
    def load_existing_data(self):
        """æ—¢å­˜ã®åŸ‹ã‚è¾¼ã¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        if os.path.exists(self.embeddings_file):
            try:
                with open(self.embeddings_file, 'r', encoding='utf-8') as f:
                    self.embeddings_data = json.load(f)
                
                # å‡¦ç†æ¸ˆã¿ç”»åƒã®ãƒãƒƒã‚·ãƒ¥ã‚’è¨˜éŒ²
                for item in self.embeddings_data:
                    if 'file_hash' in item:
                        self.processed_images.add(item['file_hash'])
                
                print(f"ğŸ“ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {len(self.embeddings_data)}ä»¶")
                print(f"ğŸ“ å‡¦ç†æ¸ˆã¿ç”»åƒ: {len(self.processed_images)}ä»¶")
            except Exception as e:
                print(f"âŒ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
                self.embeddings_data = []
        else:
            print("ğŸ“‚ æ–°è¦ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä½œæˆã—ã¾ã™")
    
    def get_file_hash(self, file_path: Path) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¨ˆç®—ï¼ˆé‡è¤‡æ¤œå‡ºç”¨ï¼‰"""
        hash_sha256 = hashlib.sha256()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_sha256.update(chunk)
        return hash_sha256.hexdigest()
    
    def image_to_base64_data_url(self, image_path: Path) -> str:
        # ğŸ†• å¤‰æ›´: ãƒªã‚µã‚¤ã‚ºå‡¦ç†ã‚’å«ã‚€ç”»åƒãƒ‡ãƒ¼ã‚¿å–å¾—
        image_data = self.resize_image_if_needed(image_path)
         # ğŸ†• å¤‰æ›´: ç›´æ¥base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ä¸è¦ï¼‰
        base64_string = base64.b64encode(image_data).decode('utf-8')
        """ç”»åƒã‚’base64ãƒ‡ãƒ¼ã‚¿URLã«å¤‰æ›"""
            
        ext = image_path.suffix.lower()
        mime_type = {
            '.jpg': 'image/jpeg',
            '.jpeg': 'image/jpeg', 
            '.png': 'image/png',
            '.webp': 'image/webp',
            '.gif': 'image/gif'
        }.get(ext, 'image/jpeg')
        
        return f"data:{mime_type};base64,{base64_string}"
    
    def resize_image_if_needed(self, image_path: Path) -> bytes:
        """
        ç”»åƒã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒ20MBã‚’è¶…ãˆã‚‹å ´åˆã€ãƒªã‚µã‚¤ã‚ºã—ã¦åˆ¶é™å†…ã«åã‚ã‚‹
        :param image_path: ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        :return: ãƒªã‚µã‚¤ã‚ºã•ã‚ŒãŸç”»åƒã®ãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿
        """
        file_size = image_path.stat().st_size
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒåˆ¶é™å†…ã®å ´åˆã¯ãã®ã¾ã¾è¿”ã™
        if file_size <= self.max_file_size:
            with open(image_path, 'rb') as f:
                return f.read()
        
        print(f"ğŸ“ å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œå‡º ({file_size / (1024*1024):.1f}MB): {image_path.name}")
        print(f"   ğŸ”„ ãƒªã‚µã‚¤ã‚ºã‚’å®Ÿè¡Œä¸­...")
        
        try:
            with Image.open(image_path) as img:
                # å…ƒã®ç”»åƒæƒ…å ±ã‚’ä¿æŒ
                original_format = img.format
                original_size = img.size
                
                # RGBAç”»åƒã®å ´åˆã¯RGBã«å¤‰æ›ï¼ˆJPEGã‚µãƒãƒ¼ãƒˆã®ãŸã‚ï¼‰
                if img.mode in ('RGBA', 'LA', 'P'):
                    # é€æ˜èƒŒæ™¯ã‚’ç™½ã«å¤‰æ›
                    background = Image.new('RGB', img.size, (255, 255, 255))
                    if img.mode == 'P':
                        img = img.convert('RGBA')
                    background.paste(img, mask=img.split()[-1] if img.mode in ('RGBA', 'LA') else None)
                    img = background
                
                # å“è³ªã‚’æ®µéšçš„ã«ä¸‹ã’ãªãŒã‚‰ãƒªã‚µã‚¤ã‚ºã‚’è©¦è¡Œ
                quality_levels = [95, 85, 75, 65, 55, 45]
                scale_factors = [0.9, 0.8, 0.7, 0.6, 0.5, 0.4]
                
                for quality in quality_levels:
                    for scale in scale_factors:
                        # æ–°ã—ã„ã‚µã‚¤ã‚ºã‚’è¨ˆç®—
                        new_width = int(original_size[0] * scale)
                        new_height = int(original_size[1] * scale)
                        
                        # ãƒªã‚µã‚¤ã‚º
                        resized_img = img.resize((new_width, new_height), Image.Resampling.LANCZOS)
                        
                        # ãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›
                        output = io.BytesIO()
                        
                        # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’æ±ºå®šï¼ˆJPEGã‚’å„ªå…ˆã—ã¦åœ§ç¸®åŠ¹ç‡ã‚’é«˜ã‚ã‚‹ï¼‰
                        save_format = 'JPEG' if original_format in ['JPEG', 'JPG'] or quality < 85 else original_format or 'JPEG'
                        
                        if save_format == 'JPEG':
                            resized_img.save(output, format=save_format, quality=quality, optimize=True)
                        else:
                            resized_img.save(output, format=save_format, optimize=True)
                        
                        resized_data = output.getvalue()
                        resized_size = len(resized_data)
                        
                        # ã‚µã‚¤ã‚ºãŒåˆ¶é™å†…ã«åã¾ã£ãŸå ´åˆ
                        if resized_size <= self.max_file_size:
                            compression_ratio = (file_size - resized_size) / file_size * 100
                            print(f"   âœ… ãƒªã‚µã‚¤ã‚ºå®Œäº†: {original_size} â†’ {new_width}x{new_height}")
                            print(f"   ğŸ“‰ ã‚µã‚¤ã‚ºå‰Šæ¸›: {file_size/(1024*1024):.1f}MB â†’ {resized_size/(1024*1024):.1f}MB ({compression_ratio:.1f}%å‰Šæ¸›)")
                            print(f"   ğŸ¯ å“è³ª: {quality}%, ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: {save_format}")
                            return resized_data
                
                # ã©ã®è¨­å®šã§ã‚‚åˆ¶é™å†…ã«åã¾ã‚‰ãªã„å ´åˆã®æœ€çµ‚æ‰‹æ®µ
                print(f"   âš ï¸  è­¦å‘Š: æœ€å¤§åœ§ç¸®ã§ã‚‚åˆ¶é™ã‚’è¶…éã€æœ€å°ã‚µã‚¤ã‚ºã§å‡¦ç†")
                final_size = (400, 300)  # æœ€å°ã‚µã‚¤ã‚º
                resized_img = img.resize(final_size, Image.Resampling.LANCZOS)
                output = io.BytesIO()
                resized_img.save(output, format='JPEG', quality=45, optimize=True)
                return output.getvalue()
                
        except Exception as e:
            print(f"   âŒ ãƒªã‚µã‚¤ã‚ºã‚¨ãƒ©ãƒ¼: {e}")
            print(f"   ğŸ“„ å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãã®ã¾ã¾ä½¿ç”¨ï¼ˆAPIã‚¨ãƒ©ãƒ¼ã®å¯èƒ½æ€§ã‚ã‚Šï¼‰")
            with open(image_path, 'rb') as f:
                return f.read()
    
    def get_image_embedding(self, image_path: Path) -> np.ndarray:
        """ç”»åƒã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆ"""
        try:
            base64_url = self.image_to_base64_data_url(image_path)
            
            image_input = {
                "content": [
                    {"type": "image_url", "image_url": {"url": base64_url}}
                ]
            }
            
            response = self.client.embed(
                model="embed-v4.0",
                inputs=[image_input],
                input_type="search_document",
                embedding_types=["float"]
            )
            
            embedding = response.embeddings.float_[0]
            return np.array(embedding)
            
        except Exception as e:
            print(f"âŒ åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã‚¨ãƒ©ãƒ¼ ({image_path.name}): {e}")
            return None

    def get_meta_embedding(self, image_path: Path) -> np.ndarray:
        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æ¤œç´¢ã‚¯ã‚¨ãƒªåŒ–
        # ä¾‹: images/details/sample.png â†’ 'images details sample.png'
        path_parts = list(image_path.parts)
        if len(path_parts) > 1:
            # æ‹¡å¼µå­ä»˜ããƒ•ã‚¡ã‚¤ãƒ«åã¯ãã®ã¾ã¾
            query_str = ' '.join(path_parts[:-1] + [image_path.name])
        else:
            query_str = image_path.name
        # ãƒ†ã‚­ã‚¹ãƒˆãƒ™ã‚¯ãƒˆãƒ«ï¼ˆæ¤œç´¢ã‚¯ã‚¨ãƒªã¨ã—ã¦ï¼‰
        try:
            response = self.client.embed(
                model="embed-v4.0",
                texts=[query_str],
                input_type="search_query",
                embedding_types=["float"]
            )
            meta_vec = np.array(response.embeddings.float_[0])
            return meta_vec
        except Exception as e:
            print(f"âŒ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆå¤±æ•—: {image_path} ({e})")
            return None
        
    def get_weighted_image_and_meta_embedding(self, image_path: Path, w: float = 0.5) -> np.ndarray:
        """
        ç”»åƒã®ã¿ã®ãƒ™ã‚¯ãƒˆãƒ«(img_vec)ã¨ã€ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æ¤œç´¢ã‚¯ã‚¨ãƒªåŒ–ã—ãŸãƒ†ã‚­ã‚¹ãƒˆãƒ™ã‚¯ãƒˆãƒ«(meta_vec)ã‚’ç”Ÿæˆã—ã€
        é‡ã¿wã§åˆæˆã—ãŸæœ€çµ‚ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¿”ã™ã€‚
        :param image_path: ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        :param w: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ†ã‚­ã‚¹ãƒˆï¼‰ãƒ™ã‚¯ãƒˆãƒ«ã®é‡ã¿ï¼ˆ0.0ã€œ1.0ï¼‰
        :return: åˆæˆãƒ™ã‚¯ãƒˆãƒ«ï¼ˆnp.ndarrayï¼‰
        """
        # ç”»åƒã®ã¿ã®ãƒ™ã‚¯ãƒˆãƒ«
        img_vec = self.get_image_embedding(image_path)
        if img_vec is None:
            print(f"âŒ ç”»åƒãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆå¤±æ•—: {image_path}")
            return None
        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æ¤œç´¢ã‚¯ã‚¨ãƒªåŒ–
        # ä¾‹: images/details/sample.png â†’ 'images details sample.png'
        meta_vec = self.get_meta_embedding(image_path)
        # åˆæˆ
        w = np.dot(img_vec, meta_vec) / (np.linalg.norm(img_vec) * np.linalg.norm(meta_vec))
        vec = w * meta_vec + (1.0 - w) * img_vec
        return vec
    
    def process_new_images(self):
        """æ–°ã—ã„ç”»åƒã®ã¿ã‚’å‡¦ç†"""
        if not self.images_dir.exists():
            raise FileNotFoundError(f"ç”»åƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª '{self.images_dir}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        image_extensions = {'.jpg', '.jpeg', '.png', '.webp', '.gif'}
        image_files = [f for f in self.images_dir.iterdir() 
                      if f.suffix.lower() in image_extensions]
        
        new_images = []
        skipped_images = []
        large_images = []  # ğŸ†• è¿½åŠ : å¤§å®¹é‡ç”»åƒãƒªã‚¹ãƒˆ
        
        # æ–°ã—ã„ç”»åƒã¨æ—¢å‡¦ç†ç”»åƒã‚’åˆ†é¡
        for image_path in image_files:
            file_hash = self.get_file_hash(image_path)
            if file_hash not in self.processed_images:
                new_images.append((image_path, file_hash))
                # ğŸ†• è¿½åŠ : å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
                if image_path.stat().st_size > self.max_file_size:
                    large_images.append(image_path)
            else:
                skipped_images.append(image_path.name)
        
        print(f"ğŸ“Š ç”»åƒåˆ†æçµæœ:")
        print(f"  ğŸ†• æ–°è¦ç”»åƒ: {len(new_images)}ä»¶")
        print(f"  ğŸ“ å¤§å®¹é‡ç”»åƒï¼ˆãƒªã‚µã‚¤ã‚ºå¯¾è±¡ï¼‰: {len(large_images)}ä»¶")  # ğŸ†• è¿½åŠ 
        print(f"  â­ï¸  ã‚¹ã‚­ãƒƒãƒ—: {len(skipped_images)}ä»¶")
        
        if skipped_images:
            print(f"  ğŸ“‚ ã‚¹ã‚­ãƒƒãƒ—ã•ã‚ŒãŸç”»åƒ: {', '.join(skipped_images)}")
        
        # ğŸ†• è¿½åŠ : å¤§å®¹é‡ç”»åƒã®è©³ç´°è¡¨ç¤º
        if large_images:
            print(f"  ğŸ”„ ãƒªã‚µã‚¤ã‚ºäºˆå®šã®ç”»åƒ:")
            for img_path in large_images:
                size_mb = img_path.stat().st_size / (1024 * 1024)
                print(f"    - {img_path.name} ({size_mb:.1f}MB)")

        if not new_images:
            print("âœ… å‡¦ç†ã™ã‚‹æ–°ã—ã„ç”»åƒã¯ã‚ã‚Šã¾ã›ã‚“")
            return
        
        # æ–°ã—ã„ç”»åƒã‚’å‡¦ç†
        processed_count = 0
        for i, (image_path, file_hash) in enumerate(new_images, 1):
            print(f"ğŸ”„ å‡¦ç†ä¸­ {i}/{len(new_images)}: {image_path.name}")
            
            # embedding = self.get_image_embedding(image_path)
            # embedding = self.get_image_and_path_embedding(image_path)
            embedding = self.get_weighted_image_and_meta_embedding(image_path)
            if embedding is not None:
                # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
                new_data = {
                    "filename": image_path.name,
                    "filepath": str(image_path),
                    "file_hash": file_hash,
                    "file_size": image_path.stat().st_size,
                    "embedding": embedding.tolist()
                }
                
                self.embeddings_data.append(new_data)
                self.processed_images.add(file_hash)
                processed_count += 1
                
                print(f"âœ… å®Œäº†: {image_path.name}")
            else:
                print(f"âŒ å¤±æ•—: {image_path.name}")
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä¿å­˜
        if processed_count > 0:
            self.save_embeddings()
            print(f"\nğŸ‰ å‡¦ç†å®Œäº†! æ–°ãŸã«{processed_count}ä»¶ã®ç”»åƒã‚’è¿½åŠ ã—ã¾ã—ãŸ")
            print(f"ğŸ“Š ç·ç”»åƒæ•°: {len(self.embeddings_data)}ä»¶")
        
    def save_embeddings(self):
        """åŸ‹ã‚è¾¼ã¿ãƒ‡ãƒ¼ã‚¿ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        try:
            with open(self.embeddings_file, 'w', encoding='utf-8') as f:
                json.dump(self.embeddings_data, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜å®Œäº†: {self.embeddings_file}")
        except Exception as e:
            print(f"âŒ ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def show_status(self):
        """ç¾åœ¨ã®çŠ¶æ³ã‚’è¡¨ç¤º"""
        print(f"\nğŸ“Š ç”»åƒå‡¦ç†ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ³:")
        print(f"  ğŸ“ ç”»åƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.images_dir}")
        print(f"  ğŸ’¾ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«: {self.embeddings_file}")
        print(f"  ğŸ“ˆ å‡¦ç†æ¸ˆã¿ç”»åƒæ•°: {len(self.embeddings_data)}ä»¶")
        print(f"  ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºä¸Šé™: {self.max_file_size / (1024*1024):.0f}MB")  # ğŸ†• è¿½åŠ 
        
        if self.embeddings_data:
            print(f"\nğŸ“‚ å‡¦ç†æ¸ˆã¿ç”»åƒä¸€è¦§:")
            for item in self.embeddings_data:
                file_size_mb = item.get('file_size', 0) / (1024 * 1024)
                print(f"  - {item['filename']} ({file_size_mb:.2f}MB)")

def main():
    print("ğŸ–¼ï¸  ç”»åƒå‡¦ç†ã‚·ã‚¹ãƒ†ãƒ  - RAGãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰")
    print("=" * 50)
    
    processor = ImageProcessor(
        images_dir="images/high_resolution", embeddings_file="embedding_dynamic_weight_high_resolution.json"
        )
    
    # ç¾åœ¨ã®çŠ¶æ³è¡¨ç¤º
    processor.show_status()
    
    # æ–°ã—ã„ç”»åƒã‚’å‡¦ç†
    print(f"\nğŸ”„ æ–°ã—ã„ç”»åƒã®å‡¦ç†ã‚’é–‹å§‹...")
    processor.process_new_images()
    
    print(f"\nâœ… ç”»åƒå‡¦ç†å®Œäº†!")
    print(f"æ¤œç´¢ã‚’é–‹å§‹ã™ã‚‹ã«ã¯: python search.py")

if __name__ == "__main__":
    main()