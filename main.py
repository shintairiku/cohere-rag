"""
ç”»åƒæ¤œç´¢ã¨ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚’æä¾›ã™ã‚‹FastAPIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã€‚

ä¸»ãªæ©Ÿèƒ½:
1. Google Driveä¸Šã®ç”»åƒãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚¸ãƒ§ãƒ–ã®å®Ÿè¡Œ
2. åŸ‹ã‚è¾¼ã¿ãƒ—ãƒ­ãƒã‚¤ãƒ€ã‚’åˆ©ç”¨ã—ãŸé¡ä¼¼ç”»åƒæ¤œç´¢
"""

import html
import os
import traceback
from typing import Dict, Optional, List, Any

import gspread
from google.oauth2 import service_account
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException, Query, Request, Response
from pydantic import BaseModel
from google.cloud import run_v2

from embedding_providers import get_embedding_provider
from search import ImageSearcher
from drive_watch import DriveWatchManager, DriveNotificationProcessor

try:
    from google.cloud import translate_v2 as translate
except ImportError:  # pragma: no cover
    translate = None

load_dotenv()


class Config:
    """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®šã‚’èª­ã¿è¾¼ã‚“ã§ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹ã€‚"""
    
    def __init__(self):
        self.gcs_bucket_name = os.getenv("GCS_BUCKET_NAME")
        self.gcp_project_id = os.getenv("GCP_PROJECT_ID")
        self.vectorize_job_name = os.getenv("VECTORIZE_JOB_NAME", "cohere-rag-vectorize-job")
        self.gcp_region = os.getenv("GCP_REGION", "asia-northeast1")
        self.vertex_multimodal_model = os.getenv("VERTEX_MULTIMODAL_MODEL", "multimodalembedding@001")
        self.embedding_provider = os.getenv("EMBEDDING_PROVIDER", "vertex_ai")
        self.cohere_api_key = os.getenv("COHERE_API_KEY", "")
        # Google Sheets ID ã¯ç’°å¢ƒå¤‰æ•°ã§ä¸Šæ›¸ãå¯èƒ½ã€‚æœªæŒ‡å®šæ™‚ã¯ ENVIRONMENT ã«å¿œã˜ã¦æ—¢å®šå€¤ã‚’é¸ã¶
        dev_sheets_id = "1xPY1w4q9wm607hNK9Eb0D5v5ub7JFRihx9d-VOpHYOo"
        prod_sheets_id = "1pxSyLLZ-G3U3wwTYNgX_Qzijv7Mzn_6xSRIxGrM9l-4"
        default_sheets_id = prod_sheets_id if os.getenv("ENVIRONMENT") == "production" else dev_sheets_id
        self.google_sheets_id = os.getenv("GOOGLE_SHEETS_ID", default_sheets_id)
        self.company_sheet_name = "ä¼šç¤¾ä¸€è¦§"
        self.drive_watch_callback_url = os.getenv("DRIVE_WEBHOOK_URL")
        ttl_value = os.getenv("DRIVE_WATCH_TTL_SECONDS", "").strip()
        self.drive_watch_ttl_seconds = int(ttl_value or "86400")
        cooldown_value = os.getenv("DRIVE_WATCH_COOLDOWN_SECONDS", "").strip()
        cooldown_seconds = int(cooldown_value or "60")
        self.drive_watch_cooldown_seconds = cooldown_seconds if cooldown_seconds >= 0 else 0
        verbose_flag = os.getenv("DRIVE_WATCH_VERBOSE_LOGS", "true").strip().lower()
        self.drive_watch_verbose_logs = verbose_flag not in {"false", "0", "no"}
        
        self._validate_required_vars()
    
    def _validate_required_vars(self):
        """å¿…é ˆã®ç’°å¢ƒå¤‰æ•°ãŒæƒã£ã¦ã„ã‚‹ã‹æ¤œè¨¼ã™ã‚‹ã€‚"""
        required_vars = [
            ("GCS_BUCKET_NAME", self.gcs_bucket_name),
            ("GCP_PROJECT_ID", self.gcp_project_id)
        ]
        
        missing_vars = [name for name, value in required_vars if not value]
        if missing_vars:
            raise RuntimeError(f"FATAL: Required environment variables are missing: {', '.join(missing_vars)}")


# Initialize configuration and clients
config = Config()
app = FastAPI(
    title="Image Search and Vectorization API",
    version="1.0.0",
    description="API for vectorizing Google Drive images and performing similarity search"
)
run_client = run_v2.JobsClient()

class VectorizeRequest(BaseModel):
    """ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã§åˆ©ç”¨ã™ã‚‹ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã€‚"""
    uuid: str
    drive_url: str
    use_embed_v4: bool = False


class VectorizeTask(BaseModel):
    """ãƒãƒƒãƒå‡¦ç†ç”¨ã®å˜ä¸€ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚¿ã‚¹ã‚¯å®šç¾©ã€‚"""
    uuid: str
    drive_url: str
    company_name: str = ""
    use_embed_v4: bool = False


class BatchVectorizeRequest(BaseModel):
    """ãƒãƒƒãƒãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã€‚"""
    tasks: List[VectorizeTask]


class SearchRequest(BaseModel):
    """æ¤œç´¢ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã€‚"""
    uuid: str
    q: Optional[str] = None
    top_k: int = 5
    trigger: str = "ã‚¹ã‚¿ãƒ³ãƒ€ãƒ¼ãƒ‰"
    exclude_files: List[str] = []
    use_embed_v4: bool = False
    top_n: Optional[int] = None
    search_model: Optional[str] = None


class DriveWatchRequest(BaseModel):
    """Google Driveã®å¤‰æ›´ç›£è¦–ãƒãƒ£ãƒãƒ«ä½œæˆç”¨ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã€‚"""
    uuid: str
    drive_url: str
    company_name: str = ""
    callback_url: Optional[str] = None
    use_embed_v4: bool = False


class CompanyState(BaseModel):
    """ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‹ã‚‰é€ä¿¡ã•ã‚Œã‚‹ä¼æ¥­è¨­å®šã€‚"""
    uuid: str
    drive_url: str
    company_name: str = ""
    use_embed_v4: bool = False


class CompanyStateBatchRequest(BaseModel):
    """ä¼æ¥­è¨­å®šã‚’ã¾ã¨ã‚ã¦ä¿å­˜ã™ã‚‹ãƒªã‚¯ã‚¨ã‚¹ãƒˆã€‚"""
    companies: List[CompanyState]


class DeleteCompanyStateResponse(BaseModel):
    """ä¼æ¥­è¨­å®šå‰Šé™¤ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã€‚"""
    uuid: str
    removed_watch: bool


class ReRegisterRequest(BaseModel):
    """ãƒãƒ£ãƒãƒ«ã®å†ç™»éŒ²ãƒªã‚¯ã‚¨ã‚¹ãƒˆã€‚"""
    uuids: Optional[List[str]] = None


class JobService:
    """Cloud Runã‚¸ãƒ§ãƒ–ã®å®Ÿè¡Œã‚’ç®¡ç†ã™ã‚‹ã‚µãƒ¼ãƒ“ã‚¹ã‚¯ãƒ©ã‚¹ã€‚"""
    
    def __init__(self, config: Config, run_client: run_v2.JobsClient):
        self.config = config
        self.run_client = run_client
    
    def _build_job_env(self, additional: List[Dict[str, str]]) -> List[Dict[str, str]]:
        env_vars = list(additional)
        env_vars.extend([
            {"name": "GCS_BUCKET_NAME", "value": self.config.gcs_bucket_name},
            {"name": "GCP_PROJECT_ID", "value": self.config.gcp_project_id},
            {"name": "GCP_REGION", "value": self.config.gcp_region},
            {"name": "VERTEX_MULTIMODAL_MODEL", "value": self.config.vertex_multimodal_model},
            {"name": "EMBEDDING_PROVIDER", "value": self.config.embedding_provider},
        ])
        if self.config.cohere_api_key:
            env_vars.append({"name": "COHERE_API_KEY", "value": self.config.cohere_api_key})
        return env_vars
    
    def trigger_vectorization_job(self, uuid: str, drive_url: str, use_embed_v4: bool = False) -> Dict:
        """
        å˜ä¸€UUIDå‘ã‘ã®Cloud Runã‚¸ãƒ§ãƒ–ã‚’èµ·å‹•ã—ã¦ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚’å®Ÿè¡Œã™ã‚‹ã€‚
        
        å¼•æ•°:
            uuid: ä¼æ¥­ã®UUID
            drive_url: ç”»åƒã‚’æ ¼ç´ã—ãŸGoogle Driveãƒ•ã‚©ãƒ«ãƒ€ã®URL
            use_embed_v4: embed-v4.0ãƒ¢ãƒ‡ãƒ«ã‚’å¼·åˆ¶ã™ã‚‹ã‹ã©ã†ã‹
            
        æˆ»ã‚Šå€¤:
            ã‚¸ãƒ§ãƒ–å®Ÿè¡Œæƒ…å ±ã‚’å«ã‚€è¾æ›¸
            
        ä¾‹å¤–:
            Exception: ã‚¸ãƒ§ãƒ–èµ·å‹•ã«å¤±æ•—ã—ãŸå ´åˆ
        """
        print(f"API: Received request to start vectorization job for UUID: {uuid}")
        
        job_parent = f"projects/{self.config.gcp_project_id}/locations/{self.config.gcp_region}"
        job_name = f"{job_parent}/jobs/{self.config.vectorize_job_name}"
        
        try:
            print(f"  -> Attempting to run job: {job_name}")
            
            request_object = run_v2.RunJobRequest(
                name=job_name,
                overrides=run_v2.RunJobRequest.Overrides(
                    container_overrides=[
                        run_v2.RunJobRequest.Overrides.ContainerOverride(
                            env=self._build_job_env(
                                additional=[
                                    {"name": "UUID", "value": uuid},
                                    {"name": "DRIVE_URL", "value": drive_url},
                                    {"name": "USE_EMBED_V4", "value": str(use_embed_v4)},
                                ]
                            )
                        )
                    ]
                )
            )
            
            response = self.run_client.run_job(request=request_object)
            
            # Extract execution info from response
            if hasattr(response, 'name'):
                execution_info = response.name
            elif hasattr(response, 'metadata'):
                execution_info = str(response.metadata)
            else:
                execution_info = f"Job triggered for {uuid}"
            
            print(f"  -> Job execution started. Info: {execution_info}")
            return {
                "message": f"Vectorization job started successfully for UUID: {uuid}",
                "execution_info": execution_info,
                "job_name": self.config.vectorize_job_name
            }
            
        except Exception as e:
            error_msg = f"Failed to start Cloud Run Job: {str(e)}"
            print(f"  -> ERROR: {error_msg}")
            traceback.print_exc()
            raise Exception(error_msg)

    def trigger_batch_vectorization_job(self, tasks: List[VectorizeTask]) -> Dict:
        """
        è¤‡æ•°UUIDã‚’ã¾ã¨ã‚ã¦å‡¦ç†ã™ã‚‹Cloud Runã‚¸ãƒ§ãƒ–ã‚’èµ·å‹•ã™ã‚‹ã€‚
        
        å¼•æ•°:
            tasks: ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚¿ã‚¹ã‚¯ã®ãƒªã‚¹ãƒˆ
            
        æˆ»ã‚Šå€¤:
            ã‚¸ãƒ§ãƒ–å®Ÿè¡Œæƒ…å ±ã‚’å«ã‚€è¾æ›¸
            
        ä¾‹å¤–:
            Exception: ã‚¸ãƒ§ãƒ–èµ·å‹•ã«å¤±æ•—ã—ãŸå ´åˆ
        """
        print(f"API: Received request to start batch vectorization job for {len(tasks)} tasks")
        
        job_parent = f"projects/{self.config.gcp_project_id}/locations/{self.config.gcp_region}"
        job_name = f"{job_parent}/jobs/{self.config.vectorize_job_name}"
        
        try:
            print(f"  -> Attempting to run batch job: {job_name}")
            
            # Serialize tasks to JSON for passing as environment variable
            import json
            tasks_json = json.dumps([task.dict() for task in tasks])
            
            request_object = run_v2.RunJobRequest(
                name=job_name,
                overrides=run_v2.RunJobRequest.Overrides(
                    container_overrides=[
                        run_v2.RunJobRequest.Overrides.ContainerOverride(
                            env=self._build_job_env(
                                additional=[
                                    {"name": "BATCH_MODE", "value": "true"},
                                    {"name": "BATCH_TASKS", "value": tasks_json},
                                ]
                            )
                        )
                    ]
                )
            )
            
            response = self.run_client.run_job(request=request_object)
            
            # Extract execution info from response
            if hasattr(response, 'name'):
                execution_info = response.name
            elif hasattr(response, 'metadata'):
                execution_info = str(response.metadata)
            else:
                execution_info = f"Batch job triggered for {len(tasks)} tasks"
            
            print(f"  -> Batch job execution started. Info: {execution_info}")
            return {
                "message": f"Batch vectorization job started successfully for {len(tasks)} tasks",
                "execution_info": execution_info,
                "job_name": self.config.vectorize_job_name,
                "task_count": len(tasks)
            }
            
        except Exception as e:
            error_msg = f"Failed to start batch Cloud Run Job: {str(e)}"
            print(f"  -> ERROR: {error_msg}")
            traceback.print_exc()
            raise Exception(error_msg)


# Initialize services
job_service = JobService(config, run_client)


def get_drive_watch_manager() -> DriveWatchManager:
    """ã‚¢ãƒ—ãƒªå…¨ä½“ã§å…±æœ‰ã™ã‚‹Driveç›£è¦–ãƒãƒãƒ¼ã‚¸ãƒ£ã‚’è¿”ã™ã€‚"""
    manager = getattr(app.state, "drive_watch_manager", None)
    if manager is None:
        manager = DriveWatchManager(
            bucket_name=config.gcs_bucket_name,
            default_callback_url=config.drive_watch_callback_url,
            ttl_seconds=config.drive_watch_ttl_seconds
        )
        app.state.drive_watch_manager = manager
    return manager


def get_drive_notification_processor() -> DriveNotificationProcessor:
    """Driveé€šçŸ¥ã®å‡¦ç†å™¨ã‚’åˆæœŸåŒ–ã—ã¦è¿”ã™ã€‚"""
    processor = getattr(app.state, "drive_notification_processor", None)
    if processor is None:
        processor = DriveNotificationProcessor(
            bucket_name=config.gcs_bucket_name,
            job_service=job_service,
            cooldown_seconds=config.drive_watch_cooldown_seconds,
            verbose_logging=config.drive_watch_verbose_logs,
        )
        app.state.drive_notification_processor = processor
    return processor


@app.post("/vectorize", status_code=202)
async def trigger_vectorization_job(request: VectorizeRequest):
    """æŒ‡å®šã•ã‚ŒãŸUUIDã®ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚¸ãƒ§ãƒ–ã‚’Cloud Runã§é–‹å§‹ã™ã‚‹ã€‚"""
    try:
        result = job_service.trigger_vectorization_job(request.uuid, request.drive_url, request.use_embed_v4)
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/vectorize-batch", status_code=202)
async def trigger_batch_vectorization_job(request: BatchVectorizeRequest):
    """è¤‡æ•°UUIDå‘ã‘ã®ãƒ™ã‚¯ãƒˆãƒ«åŒ–ãƒãƒƒãƒã‚¸ãƒ§ãƒ–ã‚’Cloud Runã§é–‹å§‹ã™ã‚‹ã€‚"""
    try:
        result = job_service.trigger_batch_vectorization_job(request.tasks)
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/drive/watch")
async def register_drive_watch(request: DriveWatchRequest):
    """Google Driveã®å¤‰æ›´é€šçŸ¥ãƒãƒ£ãƒãƒ«ã‚’ç™»éŒ²ã™ã‚‹ã€‚"""
    manager = get_drive_watch_manager()
    try:
        state = manager.create_watch(
            uuid=request.uuid,
            drive_url=request.drive_url,
            company_name=request.company_name,
            callback_url=request.callback_url,
            use_embed_v4=request.use_embed_v4
        )
        return {
            "message": f"Drive watch registered for UUID {request.uuid}",
            "channel_id": state.get("channel_id"),
            "resource_id": state.get("resource_id"),
            "expiration": state.get("expiration"),
            "drive_id": state.get("drive_id"),
            "is_new_channel": state.get("is_new_channel", False),
            "drive_channel_created": state.get("drive_channel_created", False),
        }
    except ValueError as exc:
        raise HTTPException(status_code=400, detail=str(exc))
    except Exception as exc:
        raise HTTPException(status_code=500, detail=f"Failed to register Drive watch: {exc}")


@app.delete("/drive/watch/{uuid}")
async def delete_drive_watch(uuid: str):
    """ç™»éŒ²æ¸ˆã¿ã®Driveé€šçŸ¥ãƒãƒ£ãƒãƒ«ã‚’åœæ­¢ã™ã‚‹ã€‚"""
    manager = get_drive_watch_manager()
    state = manager.stop_watch(uuid)
    if not state:
        raise HTTPException(status_code=404, detail=f"No Drive watch found for UUID {uuid}")
    return {
        "message": f"Drive watch removed for UUID {uuid}",
        "channel_id": state.get("channel_id"),
        "resource_id": state.get("resource_id")
    }


@app.post("/drive/company-states")
async def save_company_states(request: CompanyStateBatchRequest):
    """ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‹ã‚‰é€ä¿¡ã•ã‚ŒãŸä¼æ¥­è¨­å®šã‚’ä¿å­˜ã™ã‚‹ã€‚"""
    manager = get_drive_watch_manager()
    saved: List[Dict[str, Any]] = []
    errors: List[Dict[str, Any]] = []
    for company in request.companies:
        try:
            state = manager.save_company_state_only(
                uuid=company.uuid,
                drive_url=company.drive_url,
                company_name=company.company_name,
                use_embed_v4=company.use_embed_v4,
            )
            saved.append({
                "uuid": company.uuid,
                "drive_id": state.get("drive_id"),
                "folder_id": state.get("folder_id"),
            })
        except Exception as exc:
            errors.append({"uuid": company.uuid, "error": str(exc)})
    if not saved and errors:
        raise HTTPException(status_code=400, detail={"errors": errors})
    return {
        "saved_count": len(saved),
        "saved": saved,
        "error_count": len(errors),
        "errors": errors,
    }


@app.delete("/drive/company-states/{uuid}", response_model=DeleteCompanyStateResponse)
async def delete_company_state(uuid: str):
    """ä¼æ¥­è¨­å®šã¨é–¢é€£ã™ã‚‹ç´ã¥ã‘ã‚’å‰Šé™¤ã™ã‚‹ã€‚"""
    manager = get_drive_watch_manager()
    state = manager.stop_watch(uuid)
    if not state:
        raise HTTPException(status_code=404, detail=f"No company state found for UUID {uuid}")
    return DeleteCompanyStateResponse(uuid=uuid, removed_watch=state.get("drive_channel_stopped", False))


@app.post("/drive/watch/re-register")
async def re_register_drive_channels(request: Optional[ReRegisterRequest] = None):
    """æ—¢å­˜ä¼æ¥­ã®ãƒãƒ£ãƒãƒ«ã‚’å…±æœ‰ãƒ‰ãƒ©ã‚¤ãƒ–å˜ä½ã§å†ç™»éŒ²ã™ã‚‹ã€‚"""
    manager = get_drive_watch_manager()
    payload = request or ReRegisterRequest()
    try:
        result = manager.re_register_companies(payload.uuids)
        return result
    except ValueError as exc:
        raise HTTPException(status_code=400, detail=str(exc))
    except Exception as exc:
        raise HTTPException(status_code=500, detail=f"Failed to re-register Drive channels: {exc}")


@app.post("/drive/notifications", status_code=204)
async def drive_notifications(request: Request):
    """Google Drive APIã‹ã‚‰ã®pushé€šçŸ¥ã‚’å—ä¿¡ã—ã€å¿…è¦ã«å¿œã˜ã¦ã‚¸ãƒ§ãƒ–ã‚’å†å®Ÿè¡Œã™ã‚‹ã€‚"""
    channel_id = request.headers.get("x-goog-channel-id")
    resource_state = request.headers.get("x-goog-resource-state", "")
    resource_id = request.headers.get("x-goog-resource-id", "")
    changed_types = request.headers.get("x-goog-changed", "")
    if not channel_id:
        raise HTTPException(status_code=400, detail="Missing X-Goog-Channel-Id header.")

    processor = get_drive_notification_processor()
    try:
        processor.handle_notification(channel_id, resource_state, resource_id, changed_types)
    except Exception as exc:
        raise HTTPException(status_code=500, detail=f"Failed to handle Drive notification: {exc}")
    return Response(status_code=204)


class SearchService:
    """ç”»åƒæ¤œç´¢å‡¦ç†ã‚’ã¾ã¨ã‚ãŸã‚µãƒ¼ãƒ“ã‚¹ã‚¯ãƒ©ã‚¹ã€‚"""
    
    def __init__(self, config: Config):
        self.config = config
        self._translate_client = self._init_translate_client()

    def _init_translate_client(self):
        if translate is None:
            print("âš ï¸ google-cloud-translate ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ãªã„ãŸã‚ã€ã‚¯ã‚¨ãƒªç¿»è¨³ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            return None
        try:
            return translate.Client()
        except Exception as exc:
            print(f"âš ï¸ ç¿»è¨³ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {exc}")
            return None

    def _translate_query(self, query: str) -> str:
        if not query:
            return query
        if not self._translate_client:
            return query

        try:
            result = self._translate_client.translate(query, target_language="en")
            translated_text = result.get("translatedText") or ""
            translated_text = html.unescape(translated_text)
            source_lang = result.get("detectedSourceLanguage", "").lower()

            if translated_text:
                if source_lang and source_lang != "en":
                    print(f"ğŸŒ ã‚¯ã‚¨ãƒªã‚’ {source_lang} ã‹ã‚‰è‹±èªã«ç¿»è¨³ã—ã¾ã—ãŸ: '{translated_text}'")
                else:
                    print("ğŸŒ ã‚¯ã‚¨ãƒªã¯è‹±èªã¨åˆ¤æ–­ã•ã‚ŒãŸãŸã‚ã€ãã®ã¾ã¾ä½¿ç”¨ã—ã¾ã™ã€‚")
                return translated_text
        except Exception as exc:
            print(f"âš ï¸ ã‚¯ã‚¨ãƒªç¿»è¨³ã«å¤±æ•—ã—ãŸãŸã‚åŸæ–‡ã‚’ä½¿ç”¨ã—ã¾ã™: {exc}")

        return query
    
    def _resolve_search_options(
        self,
        search_model: Optional[str],
        use_embed_v4: bool,
    ) -> tuple[str, bool, Optional[str]]:
        """
        è¦æ±‚ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«åã‹ã‚‰åŸ‹ã‚è¾¼ã¿ãƒ—ãƒ­ãƒã‚¤ãƒ€ã¨ãƒ¢ãƒ‡ãƒ«è­˜åˆ¥å­ã‚’æ±ºå®šã™ã‚‹ã€‚
        (provider_name, use_embed_v4_flag, model_identifier_for_storage) ã‚’è¿”ã™ã€‚
        """
        if not search_model:
            default_provider = self.config.embedding_provider
            model_identifier = None
            return default_provider, use_embed_v4, model_identifier

        normalized = search_model.strip().lower()

        if normalized in {"vertex-ai", "vertex_ai", "vertex"}:
            return "vertex_ai", False, "vertex-ai"

        if normalized in {
            "cohere-embed-v4.0",
            "cohere_embed-v4.0",
            "embed-v4.0",
            "embed_v4.0",
        }:
            return "cohere", True, "cohere-embed-v4.0"

        if normalized in {
            "cohere-multilingual-v3.0",
            "cohere_multilingual-v3.0",
            "multilingual-v3.0",
            "multilingual_v3.0",
        }:
            return "cohere", False, "cohere-multilingual-v3.0"

        # æƒ³å®šå¤–ã®å€¤ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        print(f"âš ï¸ Unknown search_model '{search_model}', falling back to default provider.")
        default_provider = self.config.embedding_provider
        return default_provider, use_embed_v4, None

    def _embed_query(self, query: str, provider_name: str, use_embed_v4: bool):
        provider = get_embedding_provider(provider_name=provider_name)
        return provider.embed_text(text=query, use_embed_v4=use_embed_v4)
    
    def search_ranked(
        self,
        uuid: str,
        query: str,
        top_k: int,
        exclude_files: List[str] = None,
        use_embed_v4: bool = False,
        search_model: Optional[str] = None,
    ) -> Dict:
        """é¡ä¼¼åº¦ã§ã‚½ãƒ¼ãƒˆã—ãŸä¸Šä½top_kä»¶ã®çµæœã‚’è¿”ã™ã€‚"""
        print(f"ğŸ§  [STANDARD] Generating embedding for query: '{query}'")
        if exclude_files:
            print(f"ğŸ“‹ Excluding {len(exclude_files)} files from ranked search")

        provider_name, effective_use_embed_v4, model_identifier = self._resolve_search_options(
            search_model,
            use_embed_v4,
        )
        
        try:
            searcher = ImageSearcher(
                uuid=uuid,
                bucket_name=self.config.gcs_bucket_name,
                model_name=model_identifier,
            )
        except FileNotFoundError as e:
            print(f"âŒ Vector data not found: {e}")
            raise HTTPException(status_code=404, detail=f"Vector data for UUID '{uuid}' not found.")
        
        english_query = self._translate_query(query)
        query_embedding = self._embed_query(english_query, provider_name, effective_use_embed_v4)
        results = searcher.search_images(query_embedding=query_embedding, top_k=top_k, exclude_files=exclude_files)
        print(f"âœ… Standard search completed. Returning {len(results)} results")
        
        return {"query": query, "results": results}
    
    def search_shuffle(
        self,
        uuid: str,
        query: str,
        top_k: int,
        top_n: Optional[int] = None,
        exclude_files: List[str] = None,
        use_embed_v4: bool = False,
        search_model: Optional[str] = None,
    ) -> Dict:
        """ä¸Šä½å€™è£œã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ æŠ½å‡ºã—ãŸtop_kä»¶ã®çµæœã‚’è¿”ã™ã€‚"""
        print(f"ğŸ§  [SHUFFLE] Generating embedding for query: '{query}'")
        if exclude_files:
            print(f"ğŸ“‹ Excluding {len(exclude_files)} files from shuffle search")

        provider_name, effective_use_embed_v4, model_identifier = self._resolve_search_options(
            search_model,
            use_embed_v4,
        )
        
        try:
            searcher = ImageSearcher(
                uuid=uuid,
                bucket_name=self.config.gcs_bucket_name,
                model_name=model_identifier,
            )
        except FileNotFoundError as e:
            print(f"âŒ Vector data not found: {e}")
            raise HTTPException(status_code=404, detail=f"Vector data for UUID '{uuid}' not found.")
        
        english_query = self._translate_query(query)
        query_embedding = self._embed_query(english_query, provider_name, effective_use_embed_v4)
        pool_size = max(top_k * 3, 20) if top_n is None else max(top_n, top_k)
        pool = searcher.search_images(query_embedding=query_embedding, top_k=pool_size, exclude_files=exclude_files)
        
        if len(pool) <= top_k:
            chosen = pool
        else:
            import random
            indices = random.sample(range(len(pool)), k=top_k)
            indices.sort()
            chosen = [pool[i] for i in indices]
        
        print(f"âœ… Shuffle search completed. Returning {len(chosen)} results from pool size {len(pool)}")
        return {"query": query, "results": chosen}
    
    def search_random_images(
        self,
        uuid: str,
        count: int,
        exclude_files: List[str] = None,
        search_model: Optional[str] = None,
    ) -> Dict:
        """
        ç™»éŒ²æ¸ˆã¿ç”»åƒã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã«çµæœã‚’è¿”ã™ã€‚
        
        å¼•æ•°:
            uuid: ä¼æ¥­ã®UUID
            count: è¿”å´ã—ãŸã„ä»¶æ•°
            exclude_files: é™¤å¤–ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«åãƒªã‚¹ãƒˆ
            
        æˆ»ã‚Šå€¤:
            æ¤œç´¢çµæœã‚’å«ã‚€è¾æ›¸
        """
        if exclude_files:
            print(f"ğŸ“‹ Excluding {len(exclude_files)} files from random search")

        _, _, model_identifier = self._resolve_search_options(search_model, False)

        try:
            searcher = ImageSearcher(
                uuid=uuid,
                bucket_name=self.config.gcs_bucket_name,
                model_name=model_identifier,
            )
        except FileNotFoundError as e:
            print(f"âŒ Vector data not found: {e}")
            raise HTTPException(status_code=404, detail=f"Vector data for UUID '{uuid}' not found.")
        
        results = searcher.random_image_search(count=count, exclude_files=exclude_files)
        print(f"âœ… Random search completed. Returning {len(results)} results")
        
        return {"query": "ãƒ©ãƒ³ãƒ€ãƒ æ¤œç´¢", "results": results}


# Initialize services
search_service = SearchService(config)


class SheetsService:
    """Google Sheetsé€£æºã‚’æ‰±ã†ã‚µãƒ¼ãƒ“ã‚¹ã‚¯ãƒ©ã‚¹ã€‚"""
    
    def __init__(self, config: Config):
        self.config = config
        self._gc = self._get_sheets_client()
    
    def _get_sheets_client(self) -> gspread.Client:
        """ç’°å¢ƒã«å¿œã˜ãŸèªè¨¼æƒ…å ±ã§Google Sheetsã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–ã™ã‚‹ã€‚"""
        environment = os.getenv("ENVIRONMENT", "local")
        
        if environment == "production":
            import google.auth
            credentials, _ = google.auth.default(scopes=[
                'https://www.googleapis.com/auth/spreadsheets.readonly',
                'https://www.googleapis.com/auth/drive.readonly'
            ])
            return gspread.authorize(credentials)
        else:
            key_file = "config/marketing-automation-461305-2acf4965e0b0.json"
            if os.path.exists(key_file):
                credentials = service_account.Credentials.from_service_account_file(
                    key_file,
                    scopes=[
                        'https://www.googleapis.com/auth/spreadsheets.readonly',
                        'https://www.googleapis.com/auth/drive.readonly'
                    ]
                )
                return gspread.authorize(credentials)
            else:
                import google.auth
                credentials, _ = google.auth.default(scopes=[
                    'https://www.googleapis.com/auth/spreadsheets.readonly',
                    'https://www.googleapis.com/auth/drive.readonly'
                ])
                return gspread.authorize(credentials)
    
    def get_companies_for_auto_update(self) -> List[Dict]:
        """
        Google Sheetsã‹ã‚‰Drive URLã‚ã‚Šã‹ã¤ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ONã®ä¼æ¥­ã‚’æŠ½å‡ºã™ã‚‹ã€‚
        
        æˆ»ã‚Šå€¤:
            ä¼æ¥­æƒ…å ±ã‚’æ ¼ç´ã—ãŸè¾æ›¸ã®ãƒªã‚¹ãƒˆ
        """
        try:
            spreadsheet = self._gc.open_by_key(self.config.google_sheets_id)
            sheet = spreadsheet.worksheet(self.config.company_sheet_name)
            
            # Get all values from the sheet
            all_values = sheet.get_all_values()
            
            if len(all_values) < 2:  # No data rows
                print("No data found in the company sheet")
                return []
            
            data_rows = all_values[1:]
            
            companies_to_update = []
            
            for row_index, row in enumerate(data_rows, start=2):  # Start from row 2 (skip header)
                try:
                    # Assuming columns: A=UUID, B=Company Name, C=Drive URL, F=Checkbox
                    if len(row) < 6:
                        continue
                    
                    uuid = row[0].strip() if len(row) > 0 else ""
                    company_name = row[1].strip() if len(row) > 1 else ""
                    drive_url = row[2].strip() if len(row) > 2 else ""
                    checkbox_status = row[5].strip().upper() if len(row) > 5 else ""
                    
                    # Check if URL exists and checkbox is TRUE
                    if drive_url and checkbox_status == "TRUE":
                        companies_to_update.append({
                            "uuid": uuid,
                            "company_name": company_name,
                            "drive_url": drive_url,
                            "row_number": row_index,
                            "use_embed_v4": "embed-v4.0" in company_name
                        })
                        print(f"Found company for auto-update: {company_name} (UUID: {uuid})")
                
                except Exception as e:
                    print(f"Error processing row {row_index}: {e}")
                    continue
            
            print(f"Total companies found for auto-update: {len(companies_to_update)}")
            return companies_to_update
            
        except Exception as e:
            print(f"Error fetching companies from Google Sheets: {e}")
            traceback.print_exc()
            raise HTTPException(status_code=500, detail=f"Failed to fetch companies from Google Sheets: {str(e)}")


# Initialize sheets service
sheets_service = SheetsService(config)


@app.post("/auto-update")
async def auto_update_vectors():
    """
    ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ONã®ä¼æ¥­ã‚’è‡ªå‹•å–å¾—ã—ã€ãƒãƒƒãƒãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚’å®Ÿè¡Œã™ã‚‹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã€‚
    """
    try:
        print("ğŸ”„ Starting automatic vector update process...")
        
        # Get companies that need to be updated
        companies = sheets_service.get_companies_for_auto_update()
        
        if not companies:
            return {
                "message": "No companies found with enabled auto-update",
                "processed_count": 0,
                "results": []
            }
        
        results = []
        success_count = 0
        failure_count = 0
        
        # ãƒãƒƒãƒã‚¸ãƒ§ãƒ–ã¨ã—ã¦å®Ÿè¡Œ
        try:
            print(f"ğŸ¯ Triggering batch vectorization for {len(companies)} companies")
            
            # ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆã‚’ä½œæˆ
            tasks = []
            for company in companies:
                task = VectorizeTask(
                    uuid=company['uuid'],
                    drive_url=company['drive_url'],
                    company_name=company['company_name'],
                    use_embed_v4=company['use_embed_v4']
                )
                tasks.append(task)
            
            # ãƒãƒƒãƒã‚¸ãƒ§ãƒ–ã‚’å®Ÿè¡Œ
            batch_result = job_service.trigger_batch_vectorization_job(tasks)
            
            results.append({
                "status": "success",
                "message": batch_result['message'],
                "task_count": batch_result.get('task_count', len(companies)),
                "execution_info": batch_result.get('execution_info', '')
            })
            success_count = len(companies)
            
        except Exception as e:
            error_msg = f"Failed to trigger batch vectorization: {str(e)}"
            print(f"âŒ {error_msg}")
            
            results.append({
                "status": "error",
                "message": error_msg
            })
            failure_count = len(companies)
        
        print(f"âœ… Auto-update process completed. Success: {success_count}, Failures: {failure_count}")
        
        return {
            "message": f"Auto-update process completed. {success_count} successful, {failure_count} failed.",
            "processed_count": len(companies),
            "success_count": success_count,
            "failure_count": failure_count,
            "results": results
        }
        
    except Exception as e:
        print(f"âŒ Error in auto-update process: {e}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Auto-update process failed: {str(e)}")


@app.get("/search", response_model=Dict)
def search_images_api(
    uuid: str = Query(..., description="UUID of the company to search for"),
    q: Optional[str] = Query(None, description="Search query text"),
    top_k: int = Query(5, ge=1, le=50, description="Number of results to return"),
    trigger: str = Query("ã‚¹ã‚¿ãƒ³ãƒ€ãƒ¼ãƒ‰", description="Search type: 'ã‚¹ã‚¿ãƒ³ãƒ€ãƒ¼ãƒ‰' | 'ã‚·ãƒ£ãƒƒãƒ•ãƒ«' | 'ãƒ©ãƒ³ãƒ€ãƒ ' (äº’æ›: 'é¡ä¼¼ç”»åƒæ¤œç´¢'â†’ã‚·ãƒ£ãƒƒãƒ•ãƒ«)"),
    top_n: Optional[int] = Query(None, ge=1, le=200, description="Candidate pool size for shuffle mode"),
    search_model: Optional[str] = Query(None, description="Search embedding model identifier"),
):
    """æŒ‡å®šã—ãŸUUIDã®ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ã¦ç”»åƒæ¤œç´¢ã‚’å®Ÿè¡Œã™ã‚‹ã€‚"""
    print(f"ğŸ” Search API called: UUID={uuid}, trigger={trigger}, top_k={top_k}")
    if q:
        print(f"   Query: '{q}'")
    
    normalized_trigger = "ã‚·ãƒ£ãƒƒãƒ•ãƒ«" if trigger == "é¡ä¼¼ç”»åƒæ¤œç´¢" else trigger
    
    try:
        if normalized_trigger == "ã‚¹ã‚¿ãƒ³ãƒ€ãƒ¼ãƒ‰":
            if not q:
                print("âŒ Missing query parameter for standard search")
                raise HTTPException(status_code=400, detail="Query 'q' is required for standard search.")
            
            return search_service.search_ranked(uuid, q, top_k, search_model=search_model)
            
        elif normalized_trigger == "ã‚·ãƒ£ãƒƒãƒ•ãƒ«":
            if not q:
                print("âŒ Missing query parameter for shuffle search")
                raise HTTPException(status_code=400, detail="Query 'q' is required for shuffle search.")
            
            return search_service.search_shuffle(uuid, q, top_k, top_n=top_n, search_model=search_model)
            
        elif normalized_trigger == "ãƒ©ãƒ³ãƒ€ãƒ ":
            return search_service.search_random_images(uuid, top_k, search_model=search_model)
            
        else:
            print(f"âŒ Invalid trigger: {normalized_trigger}")
            raise HTTPException(status_code=400, detail=f"Invalid trigger: {normalized_trigger}")
            
    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ Unexpected error during search: {e}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"An unexpected error occurred during search: {str(e)}")


@app.post("/search", response_model=List[Dict])
def search_images_post(request: SearchRequest):
    """
    POSTãƒœãƒ‡ã‚£ã§æŒ‡å®šã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦ç”»åƒæ¤œç´¢ã‚’å®Ÿè¡Œã—ã€çµæœã‚’é…åˆ—ã§è¿”ã™ã€‚
    """
    print(f"ğŸ” Search API (POST) called: UUID={request.uuid}, trigger={request.trigger}, top_k={request.top_k}")
    if request.q:
        print(f"   Query: '{request.q}'")
    if request.exclude_files:
        print(f"   Excluding {len(request.exclude_files)} files")
    
    normalized = "ã‚·ãƒ£ãƒƒãƒ•ãƒ«" if request.trigger == "é¡ä¼¼ç”»åƒæ¤œç´¢" else request.trigger
    
    try:
        if normalized == "ã‚¹ã‚¿ãƒ³ãƒ€ãƒ¼ãƒ‰":
            if not request.q:
                print("âŒ Missing query parameter for standard search")
                raise HTTPException(status_code=400, detail="Query 'q' is required for standard search.")
            
            result = search_service.search_ranked(
                request.uuid,
                request.q,
                request.top_k,
                request.exclude_files,
                request.use_embed_v4,
                request.search_model,
            )
            return result.get("results", [])
            
        elif normalized == "ã‚·ãƒ£ãƒƒãƒ•ãƒ«":
            if not request.q:
                print("âŒ Missing query parameter for shuffle search")
                raise HTTPException(status_code=400, detail="Query 'q' is required for shuffle search.")
            
            result = search_service.search_shuffle(
                request.uuid,
                request.q,
                request.top_k,
                request.top_n,
                request.exclude_files,
                request.use_embed_v4,
                request.search_model,
            )
            return result.get("results", [])
            
        elif normalized == "ãƒ©ãƒ³ãƒ€ãƒ ":
            result = search_service.search_random_images(
                request.uuid, 
                request.top_k,
                request.exclude_files,
                request.search_model,
            )
            return result.get("results", [])
            
        else:
            print(f"âŒ Invalid trigger: {normalized}")
            raise HTTPException(status_code=400, detail=f"Invalid trigger: {normalized}")
            
    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ Unexpected error during search: {e}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"An unexpected error occurred during search: {str(e)}")


@app.get("/")
def health_check():
    """ç–é€šç¢ºèªç”¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã€‚"""
    return {"status": "ok", "service": "image-search-api", "version": "1.0.0"}
